{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 实验任务一: Transformer\n",
    "\n",
    "## **1. Transformer 编码器中 Encoder Layer 的实现**\n",
    "Encoder Layer简介\n",
    "    Encoder Layer 是 Transformer 编码器中的基本构建单元，由 多头自注意力机制（Multi-Head Self-Attention） 和 前馈全连接网络（Feed Forward Network） 组成，搭配两次残差连接与 LayerNorm，用于高效建模输入序列的上下文依赖关系和特征表达能力。\n",
    "\n",
    "\n",
    "这次我们还是使用AG News 数据集进行后续的分类任务，由于读取数据方式的改变，需要重新下载一下数据集。\n",
    "\n",
    "\n",
    "AG News 数据集简介\n",
    "\n",
    "    AG News 数据集来源于 AG's corpus of news articles，是一个大型的新闻数据集，由 Antonio Gulli 从多个新闻网站收集整理。\n",
    "    AG News 数据集包含 4 类新闻，每类 30,000 条训练数据，共 120,000 条训练样本 和 7,600 条测试样本。\n",
    "\n",
    "首先导入所需模块：\n",
    "\n",
    "\n"
   ],
   "id": "6376419e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "c80774dd3ffa1094"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "数据读取以及预处理\n",
    "\n",
    "不同于上次，这次我们使用pandas读取数据，相应的代码也有所修改。"
   ],
   "id": "19b60215ab3ff1c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# **1. 加载 AG NEWS 数据集**\n",
    "df = pd.read_csv(\"train.csv\")  # 请替换成你的文件路径\n",
    "df.columns = [\"label\", \"title\", \"description\"]  # CSV 有3列: 标签, 标题, 描述\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # 合并标题和描述作为输入文本\n",
    "df[\"label\"] = df[\"label\"] - 1  # AG NEWS 的标签是 1-4，我们转换成 0-3\n",
    "train_texts, train_labels = df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "number = int(0.3 * len(train_texts))\n",
    "train_texts, train_labels = train_texts[: number], train_labels[: number]\n",
    "\n",
    "df = pd.read_csv(\"test.csv\")  # 请替换成你的文件路径\n",
    "df.columns = [\"label\", \"title\", \"description\"]  # CSV 有3列: 标签, 标题, 描述\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # 合并标题和描述作为输入文本\n",
    "df[\"label\"] = df[\"label\"] - 1  # AG NEWS 的标签是 1-4，我们转换成 0-3\n",
    "test_texts, test_labels = df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "# **2. 加载 BERT Tokenizer**\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# **3. 处理数据**\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        return input_ids, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "unk_idx = tokenizer.unk_token_id\n",
    "\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ],
   "id": "53934bc9f7ef22dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 位置编码器\n",
    "在 Transformer 模型中，Self-Attention 是无序的，它无法感知输入序列的「位置信息」，即每个 token 在序列中的先后顺序。\n",
    "\n",
    "相比之下，RNN（通过时间步）和 CNN（通过局部感受野）天然就有顺序/位置的概念。\n",
    "\n",
    "因此，Transformer 需要为每个 token embedding 加入「位置信息」，这就是 Positional Encoding 的作用。\n",
    "\n",
    "论文《Attention is All You Need》中提出了如下的位置编码方式，使用正弦和余弦函数来构造具有不同频率的位置表示。\n",
    "\n",
    "原始公式：\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin\\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "-  $pos$：是序列中的位置$（0, 1, 2, ..., L-1）$\n",
    "-  $i$：是 embedding 的维度索引$（0 ~ d_{model}/2）$\n",
    "-  $d_{model}$：是 embedding 的总维度\n",
    "-  $10000$：是一个控制不同维度的 $sin/cos$ 波动频率的超参数\n",
    "\n",
    "\n",
    "公式进一步推导，\n",
    "原公式中：\n",
    "\n",
    "$$\n",
    "\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\n",
    "$$\n",
    "\n",
    "等价于：\n",
    "\n",
    "$$\n",
    "pos \\times \\frac{1}{10000^{\\frac{2i}{d_{model}}}}\n",
    "$$\n",
    "\n",
    "进一步展开成指数形式：\n",
    "\n",
    "$$\n",
    "= pos \\times e^{- \\log(10000) \\cdot \\frac{2i}{d_{model}}}\n",
    "$$\n",
    "\n",
    "请参考展开后的指数形式补充完下面的代码："
   ],
   "id": "99a882d68a5c1bc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#这段代码是 Transformer中的位置编码（PositionalEncoding），用于给输入的 token embedding 加入位置信息。\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # 创建一个全0的矩阵，shape = (max_len, d_model)\n",
    "        # 表示每个位置 (0 ~ max_len-1) 对应的 d_model 维位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 生成位置索引，shape = (max_len, 1)\n",
    "        # 即 position = [0, 1, 2, ..., max_len-1] 的列向量\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        # TODO 1: 计算 div_term，用于控制不同维度的 sin/cos 频率\n",
    "        # 要求: 使用 torch.exp() 实现 1 / 10000^(2i/d_model)\n",
    "        div_term = ...\n",
    "\n",
    "        # TODO 2: 给偶数维度位置编码赋值\n",
    "        # 要求: 使用 torch.sin() 完成 position * div_term，赋值给 pe 的偶数列\n",
    "        pe[:, 0::2] = ...\n",
    "\n",
    "        # TODO 3: 给奇数维度位置编码赋值\n",
    "        # 要求: 使用 torch.cos() 完成 position * div_term，赋值给 pe 的奇数列\n",
    "        pe[:, 1::2] = ...\n",
    "\n",
    "        # 将 pe 注册为 buffer（不会被训练优化）\n",
    "        # 并扩展成 (1, max_len, d_model) 方便后续和 batch 做广播\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 是输入的 embedding，shape = (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 将对应位置的 pe 加到 x 上\n",
    "        # self.pe[:, :x.size(1)] shape = (1, seq_len, d_model) 自动广播到 batch_size\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "\n",
    "        # 返回位置编码后的 embedding\n",
    "        return x"
   ],
   "id": "d6b5ddd0f9529874"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "    思考题1：为什么需要对偶数和奇数维度分别使用 sin 和 cos？\n",
    "\n",
    "### Multi-Head Self-Attention 模块\n",
    "\n",
    "下面是多头自注意力的实现，请你按照要求补全代码："
   ],
   "id": "cb4903a10ae02a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Multi-Head Self-Attention 的完整实现\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        # 保证 d_model 可以被 n_heads 整除，方便分头\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads # 每个 head 的特征维度\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 共享一个 Linear 层同时生成 Q, K, V\n",
    "        self.qkv_linear = nn.Linear(d_model, d_model * 3) # 输出为 [Q; K; V]\n",
    "\n",
    "        # 输出层，将多头的结果重新映射回 d_model 维度\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 输入 x: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # 一次性计算 Q、K、V，输出 shape = (batch_size, seq_len, 3 * d_model)\n",
    "        qkv = self.qkv_linear(x)\n",
    "\n",
    "        # 切分成 n_heads 个 head，准备 multi-head attention\n",
    "        # shape 变为 (batch_size, seq_len, n_heads, 3 * d_k)\n",
    "        qkv = qkv.view(batch_size, seq_len, self.n_heads, 3 * self.d_k)\n",
    "\n",
    "        # 调整维度顺序，变成 (batch_size, n_heads, seq_len, 3 * d_k)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # (batch_size, n_heads, seq_len, 3*d_k)\n",
    "\n",
    "        # 沿最后一个维度切成 Q, K, V，shape = (batch_size, n_heads, seq_len, d_k)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # (batch_size, n_heads, seq_len, d_k)\n",
    "\n",
    "        # TODO 1: 计算 attention scores\n",
    "        # 要求: 使用缩放点积的方式计算 (Q x K^T)，并除以 sqrt(d_k)\n",
    "        scores = ...\n",
    "\n",
    "        # mask 操作，屏蔽掉 padding 部分\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # TODO 2: 计算 attention 权重\n",
    "        # 要求: 在 seq_len 维度上使用 softmax 归一化 scores\n",
    "        attn = ...\n",
    "\n",
    "        # TODO 3: 计算加权求和后的 context\n",
    "        # 要求: 用 attn 加权 V，得到 context\n",
    "        context = ...\n",
    "\n",
    "        # 将多头拼接回去，shape = (batch_size, seq_len, n_heads * d_k) = (batch_size, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "\n",
    "        # 通过输出层，再映射回原始 d_model 维度\n",
    "        return self.fc(context)"
   ],
   "id": "6e0b85b6b712447e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "    思考题2：在 Multi-Head Self-Attention 机制中，为什么我们需要使用多个 attention head？\n",
    "\n",
    "    思考题3：为什么要用缩放因子 sqrt(d_k)？\n",
    "\n",
    "### TransformerEncoderLayer\n",
    "\n",
    "下面的代码实现了 Transformer 编码器中的一个标准 Encoder Layer，包含：\n",
    "\n",
    "“多头自注意力 + 前馈网络 + 两次残差连接 + 两次 LayerNorm” 的结构，用于对输入序列进行特征建模和上下文信息融合。\n",
    "\n",
    "请你按照要求补全代码："
   ],
   "id": "f9fc4bdd5c781ba3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        # 多头自注意力模块，输入输出维度都是 d_model\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, n_heads)\n",
    "\n",
    "        # 前馈全连接层，包含两层线性 + ReLU\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        # 第一层 LayerNorm，作用在自注意力的残差连接之后\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        # 第二层 LayerNorm，作用在前馈网络的残差连接之后\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # ------------------ 自注意力块 ------------------ #\n",
    "\n",
    "        # TODO 1: 计算多头自注意力输出 x2\n",
    "        x2 = ...\n",
    "\n",
    "        # TODO 2: 残差连接 + 第一层 LayerNorm\n",
    "        x = ...\n",
    "\n",
    "        # ------------------ 前馈神经网络块 ------------------ #\n",
    "\n",
    "        # TODO 3: 前馈全连接网络（两层 Linear + ReLU）得到 x2\n",
    "        x2 = ...\n",
    "\n",
    "        # TODO 4: 残差连接 + 第二层 LayerNorm\n",
    "        x = ...\n",
    "\n",
    "\n",
    "        return x"
   ],
   "id": "15801ddc0561891a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "    思考题4：为什么 Transformer Encoder Layer 中要在 Self-Attention 和 Feed Forward Network 之后都使用残差连接和 LayerNorm？试从“模型训练稳定性”和“特征传递”两个角度进行分析。\n",
    "\n",
    "## **2. 基于 Transformer Encoder 的文本分类器**\n",
    "\n",
    "下面，我们实现一个基于 Transformer Encoder 的文本分类器，通过 embedding、位置编码、多层 encoder 处理输入序列，最终使用 mean pooling 和全连接层完成文本的多类别分类任务。\n"
   ],
   "id": "13a2eb57037f0a6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, d_ff=256, num_layers=2, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. 定义词嵌入层（Embedding），输入为词表大小，输出为 d_model 维\n",
    "        # padding_idx 用于指定 padding token 的索引，避免其被训练\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "\n",
    "        # 2. 定义位置编码器，为 token embedding 添加位置信息\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # 3. 定义多个 TransformerEncoderLayer 叠加起来，num_layers 为层数\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "\n",
    "        # 4. 定义输出分类层，将 encoder 最终输出映射到 num_classes 维度\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)，输入为单词 ID 序列\n",
    "\n",
    "        # 1. 输入 token ID 通过 Embedding，转成 (batch_size, seq_len, d_model) 的 dense 向量\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 2. 加入位置编码，增强位置感知能力\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # 3. 创建 padding mask，shape: (batch_size, 1, 1, seq_len)\n",
    "        # mask = True 代表有效 token，False 代表 padding 位置\n",
    "        pad_mask = (x.sum(-1) != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        # 4. 依次通过多层 Encoder，每一层都会使用 pad_mask\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, pad_mask)\n",
    "\n",
    "        # 5. 对时间维度（seq_len）做 mean pooling，聚合所有位置的特征\n",
    "        out = x.mean(dim=1)  # mean pooling on seq_len\n",
    "\n",
    "        # 6. 分类输出，映射到类别数\n",
    "        return self.fc(out)"
   ],
   "id": "e3770893f21d4b61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "    思考题5：为什么在 TransformerEncoderClassifier 中，通常会在 Encoder 的输出上做 mean pooling（对 seq_len 取平均）？除了 mean pooling，你能否想到其他可以替代的 pooling 或特征聚合方式？并简要分析它们的优缺点。\n",
    "\n",
    "下面是模型的训练和测试:"
   ],
   "id": "c3a2916afbbd3177"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用 split 进行分词\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerEncoderClassifier(len(vocab)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "fac13bc3ec5f6e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "模型训练部分：",
   "id": "87ebfb827793b56f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
    "    for text, labels in loop:\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 更新tqdm进度条\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(train_dataloader)"
   ],
   "id": "9a4457ccdc5f9c56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "模型测试部分：",
   "id": "f7f940fb28765a01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(test_dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for text, labels in loop:\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            output = model(text)\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    loss = train_epoch()\n",
    "    acc = evaluate()\n",
    "    print(f'Epoch {epoch}: Loss = {loss:.4f}, Test Acc = {acc:.4f}')"
   ],
   "id": "7181ba5e93a8336a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "    思考题6：Transformer 相比传统的 RNN/CNN，优势在哪里？为什么 Transformer 更适合处理长文本？"
   ],
   "id": "91deae1f06ccff3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
