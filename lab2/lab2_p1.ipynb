{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习平台课程实验教程\n",
    "\n",
    "## 神经网络基础\n",
    "\n",
    "**目标**\n",
    "通过本次实验，你将掌握以下内容：\n",
    "\n",
    "1. 感知机、多层感知机(MLP)、激活函数的使用。\n",
    "2. 实现一个多层感知机模型，用于MNIST数据集的数字分类。\n",
    "3. 多种常用激活函数和正则化方法。\n",
    "\n",
    "------\n",
    "### **1. 感知机**\n",
    "感知机是人工神经网络中最简单的一种形式，它由输入层、输出层组成，不包含隐藏层。  \n",
    "然而，感知机只能解决线性可分的问题，无法产生非线性分割面，因此无法解决非线性问题。\n",
    "\n",
    "### **2. 多层感知机的从零实现**\n",
    "多层感知机是一种前馈人工神经网络模型，除了输入层和输出层之外还包含一个或多个隐藏层。每一层都完全连接到下一层。  \n",
    "MLP可以学习非线性的函数映射，适用于更复杂的数据模式识别任务，如图像分类。  \n",
    "首先导入所需模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST数据集是由0〜9手写数字图片和数字标签所组成的，由60000个训练样本和10000个测试样本组成，  \n",
    "其中每个样本都是一张28 * 28像素的灰度手写数字图，适合初学者进行图像分类任务。  \n",
    "加载与预处理MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数trainset应是一个实现了__len__和__getitem__方法的对象，代表整个MNIST训练数据集。\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size=64表示每个批次包含64个样本。可以根据硬件（如内存/GPU显存）调整这个值。\n",
    "# shuffle=True表示在每个epoch开始时都将训练数据集打乱顺序。\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "#对于测试集我们通常不需要打乱顺序\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考：如果不打乱训练集，会对训练结果产生什么影响？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型所需参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义激活函数，这里采用ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义ReLU函数\n",
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))\n",
    "\n",
    "# 生成一系列X, Y值作为输入\n",
    "x_values = torch.linspace(-5, 5, 100)\n",
    "y_values = relu(x_values)\n",
    "\n",
    "# 绘制图像\n",
    "plt.figure(figsize=(8, 6)) # 设置图表大小\n",
    "plt.plot(x_values.numpy(), y_values.numpy(), label='ReLU') # 绘制ReLU曲线\n",
    "plt.title('ReLU Function') # 图表标题\n",
    "plt.xlabel('Input Value') # x轴标签\n",
    "plt.ylabel('Output Value') # y轴标签\n",
    "plt.legend() # 显示图例\n",
    "plt.grid(True) # 显示网格\n",
    "plt.show() # 显示图表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X): \n",
    "    X = X.view((-1, num_inputs)) # view函数将每张原始图像改成长度为num_inputs的向量\n",
    "    H = relu(torch.matmul(X, W1) + b1)\n",
    "    return torch.matmul(H, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数，这里采用交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义优化器，这里依然采用最简单的随机梯度下降方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  \n",
    "    \"\"\"小批量随机梯度下降。\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            # 对每个参数按照其梯度以及学习率进行更新。\n",
    "            param -= lr * param.grad / batch_size\n",
    "            # 我们希望根据新的数据重新计算梯度，而不是累加之前的梯度。\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义准确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataloader, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        #TODO\n",
    "        #TODO\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 \n",
    "lr = 0.1\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            \n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                sgd(params, lr, batch_size)  #梯度下降\n",
    "            else:\n",
    "                optimizer.step()  #更新参数\n",
    "                \n",
    "            # 计算准确率 TODO\n",
    "            # 计算准确率 TODO\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train(net, trainloader, testloader, loss, num_epochs, batch_size=64, params=params, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. 多层感知机的简单实现**\n",
    "首先加载MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义MLP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens):\n",
    "        super(MLP, self).__init__()\n",
    "        self.f1 = nn.Linear(num_inputs, num_hiddens) # 输入层 -> 隐藏层\n",
    "        self.f2 = nn.Linear(num_hiddens, num_outputs) # 隐藏层 -> 输出层\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, num_inputs) # 将每张原始图像改成长度为num_inputs的向量\n",
    "        x = F.relu(self.f1(x)) # 使用PyTorch中的ReLU激活函数\n",
    "        x = self.f2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化模型、损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(num_inputs, num_outputs, num_hiddens)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义准确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataloader, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        #TODO\n",
    "        #TODO\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens):\n",
    "        super(MLP, self).__init__()\n",
    "        self.f1 = nn.Linear(num_inputs, num_hiddens) # 输入层 -> 隐藏层\n",
    "        self.f2 = nn.Linear(num_hiddens, num_outputs) # 隐藏层 -> 输出层\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, num_inputs) # 将每张原始图像改成长度为num_inputs的向量\n",
    "        x = F.relu(self.f1(x)) # 使用PyTorch中的ReLU激活函数\n",
    "        x = self.f2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            # 计算损失 TODO\n",
    "            \n",
    "            # 清除梯度 TODO\n",
    "             \n",
    "            # 反向传播 TODO\n",
    "            # 更新参数 TODO\n",
    "            \n",
    "            # 计算准确率 TODO\n",
    "            # 计算准确率 TODO\n",
    "            n += y.shape[0]\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "net = MLP(num_inputs, num_outputs, num_hiddens)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "train(net, trainloader, testloader, loss, num_epochs, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
