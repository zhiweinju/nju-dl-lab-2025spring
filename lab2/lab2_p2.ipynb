{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化和激活函数探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # 如果使用CUDA\n",
    "    torch.mps.manual_seed(seed)       # 如果使用MPS (Apple Silicon)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 确保卷积运算的确定性\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数实验\n",
    "\n",
    "激活函数是神经网络中的重要组成部分，它为网络引入非线性变换能力。本实验将探索三种常见的激活函数(ReLU、Sigmoid、Tanh)的特性及其对模型训练的影响。\n",
    "\n",
    "### 激活函数的实现与可视化\n",
    "\n",
    "首先我们实现这三个激活函数及其导数，并通过可视化来理解它们的特性：\n",
    "\n",
    "- ReLU: f(x) = max(0,x)\n",
    "- Sigmoid: f(x) = 1/(1+e^(-x))  \n",
    "- Tanh: f(x) = (e^x - e^(-x))/(e^x + e^(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_activation_function(MyReLU,MySigmoid,MyTanh):\n",
    "    x = torch.linspace(-10, 10, 1000)\n",
    "\n",
    "    # 使用自己实现的激活函数\n",
    "    my_relu = MyReLU()\n",
    "    my_sigmoid = MySigmoid()\n",
    "    my_tanh = MyTanh()\n",
    "\n",
    "    # 计算激活函数值和导数值\n",
    "    y_my_relu = my_relu(x)\n",
    "    y_my_sigmoid = my_sigmoid(x)\n",
    "    y_my_tanh = my_tanh(x)\n",
    "\n",
    "    y_my_relu_derivative = my_relu.derivative(x)\n",
    "    y_my_sigmoid_derivative = my_sigmoid.derivative(x)\n",
    "    y_my_tanh_derivative = my_tanh.derivative(x)\n",
    "    # 使用PyTorch的激活函数作为参考\n",
    "    torch_relu = nn.ReLU()\n",
    "    torch_sigmoid = nn.Sigmoid()\n",
    "    torch_tanh = nn.Tanh()\n",
    "    # 验证实现的正确性\n",
    "    y_torch_relu = torch_relu(x)\n",
    "    y_torch_sigmoid = torch_sigmoid(x)\n",
    "    y_torch_tanh = torch_tanh(x)\n",
    "\n",
    "    # 计算PyTorch激活函数的导数\n",
    "    x_torch = x.clone().requires_grad_(True)  # 使x支持求导\n",
    "\n",
    "    # ReLU导数\n",
    "    y_torch_relu = torch_relu(x_torch)\n",
    "    y_torch_relu.sum().backward()\n",
    "    y_torch_relu_derivative = x_torch.grad.clone()\n",
    "    x_torch.grad = None  # 清除梯度\n",
    "\n",
    "    # Sigmoid导数\n",
    "    y_torch_sigmoid = torch_sigmoid(x_torch)\n",
    "    y_torch_sigmoid.sum().backward()\n",
    "    y_torch_sigmoid_derivative = x_torch.grad.clone()\n",
    "    x_torch.grad = None\n",
    "\n",
    "    # Tanh导数\n",
    "    y_torch_tanh = torch_tanh(x_torch)\n",
    "    y_torch_tanh.sum().backward()\n",
    "    y_torch_tanh_derivative = x_torch.grad.clone()\n",
    "\n",
    "    assert torch.allclose(y_my_relu, y_torch_relu, rtol=1e-4, atol=1e-4), \"ReLU实现有误\"\n",
    "    assert torch.allclose(y_my_sigmoid, y_torch_sigmoid, rtol=1e-4, atol=1e-4), \"Sigmoid实现有误\"\n",
    "    assert torch.allclose(y_my_tanh, y_torch_tanh, rtol=1e-4, atol=1e-4), \"Tanh实现有误\"\n",
    "\n",
    "    assert torch.allclose(y_my_relu_derivative, y_torch_relu_derivative, rtol=1e-4, atol=1e-4), \"ReLU导数实现有误\"\n",
    "    assert torch.allclose(y_my_sigmoid_derivative, y_torch_sigmoid_derivative, rtol=1e-4, atol=1e-4), \"Sigmoid导数实现有误\"\n",
    "    assert torch.allclose(y_my_tanh_derivative, y_torch_tanh_derivative, rtol=1e-4, atol=1e-4), \"Tanh导数实现有误\"\n",
    "\n",
    "    # 绘制激活函数和导数\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # 绘制激活函数\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x.numpy(), y_my_relu.numpy(), label='ReLU')\n",
    "    plt.plot(x.numpy(), y_my_sigmoid.numpy(), label='Sigmoid')\n",
    "    plt.plot(x.numpy(), y_my_tanh.numpy(), label='Tanh')\n",
    "    plt.title('Activation Functions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 绘制导数\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x.numpy(), y_my_relu_derivative.numpy(), label='ReLU\\'')\n",
    "    plt.plot(x.numpy(), y_my_sigmoid_derivative.numpy(), label='Sigmoid\\'')\n",
    "    plt.plot(x.numpy(), y_my_tanh_derivative.numpy(), label='Tanh\\'')\n",
    "    plt.title('Derivatives of Activation Functions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y\\'')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己实现激活函数及其导数\n",
    "class MyReLU:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"实现ReLU激活函数: f(x) = max(0, x)\"\"\"\n",
    "        # TODO: 实现ReLU函数，返回x中所有元素与0的较大值\n",
    "        # TODO: 可以使用torch.maximum函数实现ReLU\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        \"\"\"ReLU的导数: f'(x) = 1 if x > 0 else 0\"\"\"\n",
    "        # TODO: 实现ReLU的导数，当x>0时为1，否则为0\n",
    "        # TODO: 可以使用torch.where函数实现ReLU的导数\n",
    "\n",
    "class MySigmoid:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"实现Sigmoid激活函数: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "        # TODO: 实现Sigmoid函数，返回x中所有元素的Sigmoid值\n",
    "        # TODO: 可以使用torch.exp函数实现e^(-x)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        \"\"\"Sigmoid的导数: f'(x) = f(x) * (1 - f(x))\"\"\"\n",
    "       # TODO: 实现Sigmoid的导数，提示：可以利用__call__方法\n",
    "\n",
    "class MyTanh:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"实现Tanh激活函数: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\"\"\"\n",
    "        # TODO: 实现Tanh函数，返回x中所有元素的Tanh值\n",
    "        # TODO: 可以使用torch.exp函数实现e^(-x)\n",
    "        \n",
    "    \n",
    "    def derivative(self, x):\n",
    "        \"\"\"Tanh的导数: f'(x) = 1 - f(x)^2\"\"\"\n",
    "        # TODO: 实现Tanh的导数，提示：可以利用__call__方法\n",
    "\n",
    "# 测试实现的激活函数\n",
    "\n",
    "check_activation_function(MyReLU,MySigmoid,MyTanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题1：为什么神经网络需要非线性激活函数？如果使用线性激活函数会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 梯度消失问题实验\n",
    "\n",
    "梯度消失是深度神经网络训练中的一个常见问题。当网络层数较深时，在反向传播过程中梯度会随着层数的增加而逐渐减小，导致靠近输入层的参数几乎无法更新。本实验将：\n",
    "\n",
    "1. 构建一个多层神经网络\n",
    "2. 分别使用不同激活函数(ReLU、Sigmoid、Tanh)\n",
    "3. 观察训练过程中各层的梯度分布\n",
    "4. 分析不同激活函数对梯度消失问题的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 梯度消失实验的网络\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation_name, num_layers=5):\n",
    "        super(Network, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # 创建很深的网络来测试梯度问题\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.Linear(100, 100))\n",
    "\n",
    "        if activation_name == 'relu': \n",
    "            self.activation = MyReLU()\n",
    "        elif activation_name == 'sigmoid':\n",
    "            self.activation = MySigmoid()\n",
    "        elif activation_name == 'tanh':\n",
    "            self.activation = MyTanh()\n",
    "        self.input_layer = nn.Linear(input_dim, 100)\n",
    "        self.output_layer = nn.Linear(100, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: 实现前向传播函数\n",
    "        # 1. 首先通过输入层\n",
    "        # 2. 应用激活函数\n",
    "        # 3. 依次通过每个隐藏层并应用激活函数\n",
    "        # 4. 最后通过输出层返回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(model):\n",
    "    \"\"\"分析模型各层的梯度分布\"\"\"\n",
    "    set_seed()\n",
    "    gradients_by_layer = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            # 计算每层梯度的统计信息\n",
    "            layer_grads = param.grad.cpu().numpy()\n",
    "            grad_mean = np.mean(np.abs(layer_grads))\n",
    "            grad_std = np.std(layer_grads)\n",
    "            gradients_by_layer.append({\n",
    "                'layer': name,\n",
    "                'mean': grad_mean,\n",
    "                'std': grad_std\n",
    "            })\n",
    "    return gradients_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size=64):\n",
    "    # 准备数据\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader,test_loader\n",
    "    \n",
    "def plot_results(results):\n",
    "    # 绘制结果\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 绘制准确率随epoch变化\n",
    "    for activation in results:\n",
    "        ax1.plot(results[activation]['acc_history'], label=f'{activation}')\n",
    "    ax1.set_title('Training Accuracy over Epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 绘制最后一个epoch的梯度分布\n",
    "    for activation in results:\n",
    "        last_grads = results[activation]['grad_history'][-1]\n",
    "        ax2.semilogy(last_grads, label=f'{activation}')\n",
    "    ax2.set_title('Gradient Distribution (Last Epoch)')\n",
    "    ax2.set_xlabel('Layer Index')\n",
    "    ax2.set_ylabel('Gradient Mean (log scale)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment_gradient_vanishing():\n",
    "    \"\"\"梯度消失实验\"\"\"\n",
    "    set_seed()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 训练不同激活函数的深层网络\n",
    "    results = {activation: {'acc_history': [], 'grad_history': []} for activation in ['relu', 'sigmoid', 'tanh']}\n",
    "    \n",
    "    n_epochs = 5\n",
    "    for activation in ['relu','sigmoid', 'tanh']:\n",
    "        train_loader,_ = get_data()\n",
    "        model = Network(input_dim=784, output_dim=10, activation_name=activation,num_layers=2).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # 训练阶段\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                data = data.view(data.size(0), -1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                # 收集梯度信息\n",
    "                if batch_idx % 1 == 0:\n",
    "                    gradients = analyze_gradients(model)\n",
    "                    results[activation]['grad_history'].append(\n",
    "                        [g['mean'] for g in gradients]\n",
    "                    )\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # 计算准确率\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                total += target.size(0)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # 计算epoch的平均准确率\n",
    "            epoch_acc = 100. * correct / total\n",
    "            results[activation]['acc_history'].append(epoch_acc)\n",
    "            print(f'Activation: {activation}, Epoch: {epoch}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "    plot_results(results)\n",
    "\n",
    "experiment_gradient_vanishing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题2：观察实验结果，为什么训练准确率会和激活函数选择相关？这与梯度分布有什么关系？\n",
    "\n",
    "提示：\n",
    "- 比较不同激活函数的梯度范围\n",
    "- 分析梯度消失对模型训练的影响\n",
    "- 思考为什么ReLU在深度学习中更受欢迎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU死亡现象实验\n",
    "\n",
    "ReLU死亡现象指神经元在训练过程中持续输出0，导致参数无法更新的问题。本实验将：\n",
    "\n",
    "1. 统计使用ReLU和Tanh激活函数时的神经元激活情况\n",
    "2. 分析ReLU死亡现象的产生原因\n",
    "3. 探讨如何缓解这一问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_relu_death():\n",
    "    \"\"\"ReLU死亡现象实验\"\"\"\n",
    "    set_seed()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 比较ReLU和LeakyReLU\n",
    "    activation_counts = {'relu': [], 'tanh': []}\n",
    "    \n",
    "    for activation in ['relu', 'tanh']:\n",
    "        model = Network(input_dim=784, output_dim=10, activation_name=activation, num_layers=2).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # 训练几个epoch并记录激活值为0的神经元数量\n",
    "        for epoch in range(5):\n",
    "            zero_activations = 0\n",
    "            total_activations = 0\n",
    "            train_loader,_ = get_data()\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                data = data.view(data.size(0), -1)\n",
    "                optimizer.zero_grad()\n",
    "                # 统计死亡神经元\n",
    "                x = model.input_layer(data)\n",
    "                x = model.activation(x)\n",
    "                \n",
    "                # 记录中间层的激活值\n",
    "                for layer in model.layers:\n",
    "                    x = layer(x)\n",
    "                    activated = model.activation(x)\n",
    "                    \n",
    "                    # 统计激活值为0的神经元数量\n",
    "                    if activation in ['relu', 'tanh']:\n",
    "                        zero_activations += torch.sum(activated == 0).item()\n",
    "                        total_activations += activated.numel()\n",
    "                \n",
    "                # 继续前向传播完成训练\n",
    "                output = model.output_layer(x)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # 计算死亡率\n",
    "            death_rate = zero_activations / total_activations\n",
    "            print(f'Activation: {activation}, Epoch {epoch}: Death Rate: {death_rate:.2f}%')\n",
    "            activation_counts[activation].append(death_rate)\n",
    "    \n",
    "    # 绘制死亡率随时间变化\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for activation, rates in activation_counts.items():\n",
    "        plt.plot(rates, label=activation)\n",
    "    plt.title('ReLU Death Rate During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Death Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "experiment_relu_death()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题3：ReLU死亡现象的成因是什么？有哪些解决方案？\n",
    "\n",
    "提示：\n",
    "- 分析什么情况下神经元会停止更新\n",
    "- 思考学习率、初始化方式的影响\n",
    "- 了解LeakyReLU等变体的优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 正则化方法实验\n",
    "\n",
    "过拟合是机器学习中的常见问题，正则化是缓解过拟合的重要手段。本实验将探索两种主要的正则化方法：L2正则化和Dropout。\n",
    "\n",
    "### L2正则化实验\n",
    "\n",
    "L2正则化通过在损失函数中添加权重的平方项来限制模型复杂度。本实验将：\n",
    "\n",
    "1. 构造一个容易过拟合的数据集\n",
    "2. 实现带有L2正则化的模型训练\n",
    "3. 对比有无正则化的训练效果\n",
    "4. 分析L2正则化对模型参数的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2正则化\n",
    "\n",
    "**1. 基本概念**\n",
    "L2正则化(也称为权重衰减)是深度学习中最常用的正则化技术之一。其核心思想是通过限制模型参数的大小来降低模型复杂度，从而防止过拟合。\n",
    "\n",
    "**2. 数学表达**\n",
    "\n",
    "在原始损失函数基础上添加L2正则项：\n",
    "\n",
    "$$L_{total}(\\mathbf{w}, b) = L_{original}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "其中：\n",
    "- $L_{original}$ 是原始损失函数(如均方误差)\n",
    "- $\\|\\mathbf{w}\\|^2$ 是权重向量的L2范数平方\n",
    "- $\\lambda$ 是正则化系数，控制正则化的强度\n",
    "\n",
    "**3. 工作原理**\n",
    "\n",
    "1. **参数惩罚**：\n",
    "   - L2正则化通过惩罚较大的权重参数，鼓励模型学习更小的权重值\n",
    "   - 较大的权重往往意味着模型对输入特征的依赖程度更高，更容易过拟合\n",
    "\n",
    "2. **平滑效果**：\n",
    "   - L2正则化倾向于将权重均匀分散到所有特征上\n",
    "   - 这使得模型不会过分依赖某些特定特征，提高了泛化能力\n",
    "\n",
    "3. **梯度更新**：\n",
    "   - 在参数更新时，L2正则化项的梯度为 $\\lambda\\mathbf{w}$\n",
    "   - 这相当于在每次更新时将权重缩小一个比例，故称为权重衰减\n",
    "\n",
    "**4. 与L1正则化的对比**\n",
    "\n",
    "- L2正则化倾向于产生值较小但非零的权重，权重呈现正态分布\n",
    "- L1正则化倾向于产生稀疏的权重向量，即许多权重为零\n",
    "- L2正则化在特征之间有关联时更适用，而L1正则化更适合特征选择\n",
    "\n",
    "**5. 实践应用**\n",
    "\n",
    "- 正则化系数 $\\lambda$ 是一个需要调整的超参数\n",
    "- $\\lambda = 0$ 时相当于无正则化\n",
    "- $\\lambda$ 越大，正则化效果越强，模型越简单，但可能欠拟合\n",
    "- 通常通过交叉验证来选择合适的 $\\lambda$ 值\n",
    "\n",
    "#### 数据集定义\n",
    "给定$x$，我们将**使用以下三阶多项式来生成训练和测试数据的标签：**\n",
    "\n",
    "**$$y = 0.05 + \\sum_{i = 1}^d 0.01 x_i + \\epsilon \\text{ where }\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.01^2).$$**\n",
    "\n",
    "我们选择标签是关于输入的线性函数。\n",
    "标签同时被均值为0，标准差为0.01高斯噪声破坏。\n",
    "为了使过拟合的效果更加明显，我们可以将问题的维数增加到$d = 200$，\n",
    "并使用一个只包含20个样本的小训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度和样本数\n",
    "d = 200  # 特征维度\n",
    "n_train, n_test = 20, 100  # 训练和测试数据集大小\n",
    "\n",
    "# 生成特征数据\n",
    "features = np.random.normal(size=(n_train + n_test, d))\n",
    "\n",
    "# 生成标签\n",
    "# y = 0.05 + 0.01 * sum(x_i) + epsilon\n",
    "labels = 0.05 + 0.01 * np.sum(features, axis=1)\n",
    "# 添加噪声 epsilon ~ N(0, 0.01^2)\n",
    "labels += np.random.normal(0, 0.01, size=labels.shape)\n",
    "\n",
    "# 转换为tensor\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# 分割训练集和测试集\n",
    "train_features = features[:n_train]\n",
    "test_features = features[n_train:]\n",
    "train_labels = labels[:n_train]\n",
    "test_labels = labels[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **定义$L_2$范数惩罚**\n",
    "\n",
    "实现这一惩罚最方便的方法是对所有项求平方后并将它们求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    # TODO: 使用torch.sum和.pow方法实现L2范数惩罚\n",
    "    # 提示: 对权重参数w平方求和，再除以2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对模型进行训练和测试\n",
    "\n",
    "让我们**实现一个函数来评估模型在给定数据集上的损失**，及**训练函数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def evaluate_loss(net, data_iter, loss):\n",
    "    \"\"\"评估给定数据集上模型的损失\"\"\"\n",
    "    metric = Accumulator(2)  # 损失的总和,样本数量\n",
    "    for X, y in data_iter:\n",
    "        out = net(X)\n",
    "        y = y.reshape(out.shape)\n",
    "        l = loss(out, y)\n",
    "        metric.add(l.sum(), l.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train_epoch(net, train_iter, loss, trainer, penalty_lambda):\n",
    "    \"\"\"训练模型一个epoch\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    net.train()\n",
    "    # 训练损失总和、训练样本数、实例数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        trainer.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        weight = net[0].weight\n",
    "        l = loss(y_hat, y.reshape(y_hat.shape)) + penalty_lambda * l2_penalty(weight)\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        metric.add(float(l.sum()), y.numel(), 1)\n",
    "    # 返回训练损失和训练准确率\n",
    "    return metric[0] / metric[2]\n",
    "\n",
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=100, penalty_lambda=0):\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    input_shape = train_features.shape[-1]\n",
    "    # 不设置偏置，因为我们已经在多项式中实现了它\n",
    "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    \n",
    "    # 创建数据迭代器\n",
    "    train_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(train_features, train_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    test_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(test_features, test_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.003)\n",
    "    \n",
    "    # 记录训练过程\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(net, train_iter, loss, trainer, penalty_lambda)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            train_l = evaluate_loss(net, train_iter, loss)\n",
    "            test_l = evaluate_loss(net, test_iter, loss)\n",
    "            train_loss.append(train_l)\n",
    "            test_loss.append(test_l)\n",
    "            print(f'epoch {epoch+1}, train loss {train_l:.3f}, test loss {test_l:.3f}')\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(range(1, len(train_loss) * 20 + 1, 20), train_loss, label='train')\n",
    "    plt.semilogy(range(1, len(test_loss) * 20 + 1, 20), test_loss, label='test')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print('weight:', net[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不使用正则化\n",
    "train(train_features, test_features,\n",
    "      train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用L2正则化\n",
    "train(train_features, test_features,\n",
    "      train_labels, test_labels, penalty_lambda=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简洁实现\n",
    "\n",
    "在深度学习框架中，我们无需实现L2正则化，只需要在损失函数中添加正则项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(net, train_iter, loss, trainer):\n",
    "    \"\"\"训练模型一个epoch\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    net.train()\n",
    "    # 训练损失总和、训练样本数、实例数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        trainer.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        weight = net[0].weight\n",
    "        l = loss(y_hat, y.reshape(y_hat.shape))\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        metric.add(float(l.sum()), y.numel(), 1)\n",
    "    # 返回训练损失和训练准确率\n",
    "    return metric[0] / metric[2]\n",
    "\n",
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=100, penalty_lambda=0):\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    input_shape = train_features.shape[-1]\n",
    "    # 不设置偏置，因为我们已经在多项式中实现了它\n",
    "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    \n",
    "    # 创建数据迭代器\n",
    "    train_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(train_features, train_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    test_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(test_features, test_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.003, weight_decay=penalty_lambda)\n",
    "    \n",
    "    # 记录训练过程\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(net, train_iter, loss, trainer)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            train_l = evaluate_loss(net, train_iter, loss)\n",
    "            test_l = evaluate_loss(net, test_iter, loss)\n",
    "            train_loss.append(train_l)\n",
    "            test_loss.append(test_l)\n",
    "            print(f'epoch {epoch+1}, train loss {train_l:.3f}, test loss {test_l:.3f}')\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(range(1, len(train_loss) * 20 + 1, 20), train_loss, label='train')\n",
    "    plt.semilogy(range(1, len(test_loss) * 20 + 1, 20), test_loss, label='test')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print('weight:', net[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用L2正则化\n",
    "train(train_features, test_features,\n",
    "      train_labels, test_labels, penalty_lambda=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题4：使用L2正则化后，模型的参数会发生什么变化？为什么这种变化有助于防止过拟合？\n",
    "\n",
    "提示：\n",
    "- 观察权重的数值分布变化\n",
    "- 分析正则化系数λ的影响\n",
    "- 思考为什么较小的权重有助于泛化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "*Dropout* 是一种在深度学习中广泛使用的正则化技术。\n",
    "与L2正则化通过限制权重大小来防止过拟合不同，\n",
    "Dropout通过在训练过程中随机\"丢弃\"（设置为零）神经元来实现正则化。\n",
    "\n",
    "在训练过程中，对于每个样本，每一层的每个神经元都有概率 $p$ 被暂时从网络中移除。\n",
    "具体来说，如果一个神经元的输出为 $h$，则：\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "\\frac{h}{1-p} & \\text{概率 } 1-p \\text{ (保留)} \\\\\n",
    "0 & \\text{概率 } p \\text{ (丢弃)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "这里的 $\\frac{1}{1-p}$ 是一个缩放因子，用于保持输出的期望值不变。\n",
    "\n",
    "Dropout 可以被理解为在训练时集成了多个不同的子网络。\n",
    "每次前向传播时，由于随机丢弃了一些神经元，\n",
    "实际上是在训练一个新的子网络。\n",
    "这些子网络共享参数，但结构各不相同。\n",
    "在测试时，我们使用完整的网络，但会将所有权重乘以 $(1-p)$ \n",
    "来补偿训练时的缩放。\n",
    "\n",
    "Dropout 的主要优点包括：\n",
    "\n",
    "1. 防止神经元的共适应性（Co-adaptation）\n",
    "   - 因为神经元不能依赖于特定的其他神经元的存在\n",
    "   - 必须学会与随机的神经元子集一起工作\n",
    "\n",
    "2. 提供了一种廉价的模型集成方法\n",
    "   - 每次使用Dropout相当于训练一个新的网络架构\n",
    "   - 最终模型可以看作是多个子网络的集成\n",
    "\n",
    "3. 减少神经元之间的复杂共适应关系\n",
    "   - 每个神经元必须学会更鲁棒的特征\n",
    "   - 不能过分依赖某些特定的特征组合\n",
    "\n",
    "在实践中，Dropout通常在全连接层中使用，\n",
    "丢弃概率 $p$ 通常设置为0.5，\n",
    "而在卷积层中较少使用或使用较小的丢弃概率（如0.1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    device = X.device\n",
    "    # TODO: 实现dropout层\n",
    "    # 1. 如果dropout=1，返回全0张量\n",
    "    # 2. 如果dropout=0，直接返回输入X\n",
    "    # 3. 否则，生成一个与X形状相同的随机掩码(mask)\n",
    "    #    - 使用torch.rand生成随机数，并与dropout比较创建二元掩码\n",
    "    #    - 将X与掩码相乘，并除以(1-dropout)进行缩放\n",
    "    # 注意：过程中device参数需要与X的设备相同\n",
    "    # 在本情况中，所有元素都被丢弃\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(X, device=device)\n",
    "    # 在本情况中，所有元素都被保留\n",
    "    if dropout == 0:\n",
    "        # TODO: 实现dropout=0的情况\n",
    "    # TODO: 实现dropout=其他值的情况\n",
    "    mask = \n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过下面几个例子来**测试`dropout_layer`函数**。\n",
    "我们将输入`X`通过dropout操作，dropout概率分别为0、0.5和1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
    "print(X)\n",
    "print(dropout_layer(X, 0.))\n",
    "print(dropout_layer(X, 0.5))\n",
    "print(dropout_layer(X, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型\n",
    "\n",
    "我们可以将dropout应用于每个隐藏层的输出（在激活函数之后），并且dropout只在训练期间有效。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(RegularizedNetwork, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = True\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.training:\n",
    "            x = dropout_layer(x, self.dropout_rate)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment_regularization():\n",
    "    \"\"\"比较不同正则化方法的效果\"\"\"\n",
    "    set_seed()\n",
    "    # 实验配置\n",
    "    configs = {\n",
    "        'No Regularization': {'dropout': 0.0},\n",
    "        'Dropout': {'dropout': 0.3},\n",
    "    }\n",
    "    \n",
    "    results = {name: {'train_acc': [], 'test_acc': []} for name in configs.keys()}\n",
    "    n_epochs = 15\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\nTraining with {name}\")\n",
    "        model = RegularizedNetwork(dropout_rate=config['dropout']).to(device)\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(),lr=0.5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader,test_loader = get_data(batch_size=256)\n",
    "        for epoch in range(n_epochs):\n",
    "            # 训练阶段\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                train_total += target.size(0)\n",
    "            \n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            \n",
    "            # 测试阶段\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                    test_total += target.size(0)\n",
    "            \n",
    "            test_acc = 100. * test_correct / test_total\n",
    "            \n",
    "            # 记录结果\n",
    "            results[name]['train_acc'].append(train_acc)\n",
    "            results[name]['test_acc'].append(test_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 训练准确率\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name in results:\n",
    "        plt.plot(results[name]['train_acc'], label=name)\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # 测试准确率\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name in results:\n",
    "        plt.plot(results[name]['test_acc'], label=name)\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 运行实验\n",
    "experiment_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch简洁实现\n",
    "\n",
    "dropout可以作为`nn.Module`类的一个模块来使用，\n",
    "\n",
    "```python\n",
    "nn.Dropout(dropout_rate)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(RegularizedNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate>0 else None\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "experiment_regularization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题5：Dropout为什么能够起到正则化的作用？训练时和测试时的差异处理有什么意义？\n",
    "\n",
    "提示：\n",
    "- 分析Dropout的集成学习观点\n",
    "- 思考为什么要进行比例缩放\n",
    "- 考虑Dropout对特征依赖的影响"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
