{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 实验任务一： 词嵌入\n",
    "\n",
    "## 词嵌入\n",
    "\n",
    "### **1. 词嵌入**\n",
    "\"词嵌入简介\"\n",
    "\n",
    "    词嵌入是指用一个低维向量来表示单词。词嵌入被用作自然语言处理任务（如情感分类、问答、翻译等）的基本组成部分。因此，在本次实验中，我们了解词嵌入的构造并且直观感受词嵌入。\n",
    "\n",
    "本次实验所用的词嵌入和数据集下载链接如下：\n",
    "#### 词嵌入下载链接：\n",
    "\n",
    "https://box.nju.edu.cn/d/591925358e264f3b9a75/\n",
    "\n",
    "\n",
    "#### ag数据下载链接：\n",
    "\n",
    "https://box.nju.edu.cn/f/7d3e4fce48fb446884c9/?dl=1\n",
    "\n",
    "\n",
    "### **2. 探索词嵌入**\n",
    "在本节，我们将基于训练好的Glove词嵌入(感兴趣的同学可以自行google Glove的论文，GloVe: Global Vectors for Word Representation)进行一些初步探索。首先加载Glove词嵌入\n",
    "\n",
    "\n"
   ],
   "id": "6376419e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(glove_file, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    读取 GloVe 词向量文件，并返回：\n",
    "    - word_to_vec: 单词到向量的映射\n",
    "    - word_to_index: 单词到索引的映射\n",
    "    - index_to_word: 索引到单词的映射\n",
    "    - embedding_matrix: 词嵌入矩阵\n",
    "    \"\"\"\n",
    "    word_to_vec = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "\n",
    "    # 读取 GloVe 词向量文件\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]  # 取出单词\n",
    "            vector = np.array(values[1:], dtype=np.float32)  # 取出向量\n",
    "            word_to_vec[word] = vector\n",
    "            word_to_index[word] = idx + 1  # 从1开始编号\n",
    "            index_to_word[idx + 1] = word\n",
    "\n",
    "    # 创建嵌入矩阵 (词汇大小 x 维度)\n",
    "    vocab_size = len(word_to_vec) + 1  # +1 是因为从1开始编号\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "    for word, idx in word_to_index.items():\n",
    "        embedding_matrix[idx] = word_to_vec[word]\n",
    "\n",
    "    return word_to_vec, word_to_index, index_to_word, embedding_matrix\n",
    "\n",
    "# 使用示例（请替换 'glove.6B.50d.txt' 为你的GloVe文件路径）\n",
    "glove_path = \"glove.6B.50d.txt\"\n",
    "word_to_vec, word_to_index, index_to_word, embedding_matrix = load_glove_embeddings(glove_path)\n",
    "# 示例：查看 'king' 的词向量\n",
    "print(\"king 的词向量：\", word_to_vec.get(\"king\"))"
   ],
   "id": "a6f99009670c55bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **2.1 寻找相似的词**\n",
    "请在词汇表中寻找跟king最相似的10个单词并打印这两个单词间的相似度，可以使用余弦相似度度量两个单词的相似性。\n"
   ],
   "id": "4ffcba622bb72785"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_top_similar_words(target_word, word_to_vec, top_n=5):\n",
    "    \"\"\"\n",
    "    找到离 target_word 最近的 top_n 个单词（基于余弦相似度）\n",
    "\n",
    "    :param target_word: 目标单词\n",
    "    :param word_to_vec: 词向量字典 {word: vector}\n",
    "    :param top_n: 返回最相近的单词数\n",
    "    :return: [(word, similarity)] 排序后的列表\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# 查找 \"king\" 最相似的 20 个单词\n",
    "top_words = find_top_similar_words(\"king\", word_to_vec, top_n=5)\n",
    "\n",
    "# 打印结果\n",
    "for word, sim in top_words:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ],
   "id": "1b0b1b3ac10ce78f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **2.2 多义词**\n",
    "有一些词往往具有多个意思比如苹果。请先思考一个多义词，并且使用Glove词嵌入进行验证。即Glove中与其最相似的20个单词中是否包含这两个意思的相关单词。最后，请给出这个单词并且打印跟其最相似的20个单词的相似度。\n"
   ],
   "id": "51b59b4b171cc13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_top_similar_words(target_word, word_to_vec, top_n=5):\n",
    "    \"\"\"\n",
    "    找到离 target_word 最近的 top_n 个单词（基于余弦相似度）\n",
    "\n",
    "    :param target_word: 目标单词\n",
    "    :param word_to_vec: 词向量字典 {word: vector}\n",
    "    :param top_n: 返回最相近的单词数\n",
    "    :return: [(word, similarity)] 排序后的列表\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# 查找 \"king\" 最相似的 20 个单词\n",
    "top_words = find_top_similar_words(\"king\", word_to_vec, top_n=20)\n",
    "\n",
    "# 打印结果\n",
    "for word, sim in top_words:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ],
   "id": "50b864005557fe71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **2.3 使用词嵌入表示关系(类比)**\n",
    "有一个著名的例子是: 国王的词嵌入-男人的词嵌入约等于女王的词嵌入-女人的词嵌入，即embedding(国王)-embedding(男人)≈embedding(女王)-embedding(女人)。\n"
   ],
   "id": "5eccc2ed7a0cccdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "基于这个案例，我们可以用embedding(china)-embedding(beijing)定义首都的关系。请基于中国-北京的得到的首都关系向量，找出英国的首都。英国使用2个单词england和britain进行探索，并且打印出相似度最高的10个单词。再用类似的方式找出伦敦(london)为首都对应的国家。\n",
   "id": "7861d2fa64d2df96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_top_similar_embeddings(target_embedding, word_to_vec, top_n=10):\n",
    "    \"\"\"\n",
    "    根据一个词向量，找到最相似的 top_n 个单词（基于余弦相似度）\n",
    "\n",
    "    :param target_embedding: 目标词向量 (numpy 数组)\n",
    "    :param word_to_vec: 词向量字典 {word: vector}\n",
    "    :param top_n: 返回最相近的单词数\n",
    "    :return: [(word, similarity)] 排序后的列表\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "\n",
    "    # 遍历所有单词，计算余弦相似度\n",
    "    for word, vec in word_to_vec.items():\n",
    "        similarity = 1 - cosine(target_embedding, vec)  # 余弦相似度\n",
    "        similarities.append((word, similarity))\n",
    "\n",
    "    # 按相似度排序（降序）\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[:top_n]\n",
    "#  获得以下单词的词嵌入\n",
    "england_, china_, beijing_ = word_to_vec.get(\"england\"), word_to_vec.get(\"china\"),  word_to_vec.get(\"beijing\")\n"
   ],
   "id": "733d5c5877cd372f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **2.4 词嵌入的不足**\n",
    "请叙述glove词嵌入的不足。言之有理即可，但避免出现一些比较大的阐述且没有分析，如性能一般，训练语料较少等。\n",
    "\n",
    "### **3. 使用词嵌入进行文本分类**\n",
    "我们接下来将基于Glove词嵌入对AG News数据集进行文本分类。\n",
    "\n",
    "\"AG News 数据集简介\"\n",
    "\n",
    "    AG News 数据集来源于 AG's corpus of news articles，是一个大型的新闻数据集，由 Antonio Gulli 从多个新闻网站收集整理。\n",
    "    AG News 数据集包含 4 类新闻，每类 30,000 条训练数据，共 120,000 条训练样本 和 7,600 条测试样本。\n",
    "\n",
    "#### **3.1 文本预处理**\n",
    "首先导入所需模块：\n",
    "\n",
    "可能需要安装datasets包"
   ],
   "id": "ea9fbe5900962527"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install datasets",
   "id": "c3a88f3551c63105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "id": "6e290a8535cc15e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们从AG News 数据集中加载文本。这是一个较小的语料库，有150000多个单词，但足够我们小试牛刀.\n",
   "id": "f58af48524b51a51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path = \"ag_news文件夹保存路径\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# 提取所有文本数据和标签\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "train_y = [item['label'] for item in dataset['train']]\n",
    "test_text = [item['text'] for item in dataset['test']]\n",
    "test_y = [item['label'] for item in dataset['test']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "id": "f536fdf40838f7e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "词元化\n",
    "下面的tokenize函数将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。这里我们使用之前Glove中定义过的word_to_index。\n",
    "\n",
    "在这些步骤后，我们顺序地把一段文本映射成了数字，可以送入模型中进行处理。\n"
   ],
   "id": "76d20de210c7a4a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用 split 进行分词\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def numericalize(text):\n",
    "    return torch.tensor([word_to_index.get(word, 0) for word in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "\n",
    "def pad_tensor(tensor, target_length=100, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads a tensor with the given pad_value up to target_length.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input 1D tensor.\n",
    "        target_length (int): Desired length after padding. Default is 100.\n",
    "        pad_value (int): Value to pad with. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (target_length,).\n",
    "    \"\"\"\n",
    "    current_length = tensor.size(0)\n",
    "    if current_length >= target_length:\n",
    "        return tensor[:target_length]  # Truncate if longer\n",
    "    else:\n",
    "        padding = torch.full((target_length - current_length,), pad_value, dtype=tensor.dtype)\n",
    "        return torch.cat((tensor, padding), dim=0)\n",
    "\n",
    "# 生成训练数据\n",
    "def create_data(text_list, seq_len=100):\n",
    "    X = []\n",
    "    for text in text_list:\n",
    "        token_ids = numericalize(text)\n",
    "        # 都处理成长度为100的序列\n",
    "        token_ids = pad_tensor(token_ids)\n",
    "        X.append(token_ids)\n",
    "    return torch.stack(X)\n",
    "\n",
    "\n",
    "# 生成训练数据\n",
    "X_train = create_data(train_text, seq_len=100)\n",
    "Y_train = torch.Tensor(train_y)\n",
    "\n",
    "# 生成测试数据\n",
    "X_test = create_data(test_text, seq_len=100)\n",
    "Y_test = torch.Tensor(test_y)\n",
    "\n",
    "# 考虑到训练时间 只取前 50% 的数据\n",
    "subset_size = int(0.5 * len(X_train))  # 计算 50% 的样本数量\n",
    "X_train = X_train[:subset_size]\n",
    "Y_train = Y_train[:subset_size]\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ],
   "id": "7020798abb45450b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.2 嵌入层**\n",
    "nn.Embedding()是 PyTorch 中用于创建词嵌入层（embedding layer）的模块，通常用于自然语言处理（NLP）任务。它的主要功能是将单词索引映射为稠密的向量表示。\n",
    "\n",
    "在本次实验中，我们使用self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)，其中参数embedding_matrix是Glove的词嵌入，即我们使用Glove的词嵌入来初始化嵌入层；参数freeze表示嵌入层是否会更新参数，我们设置为freeze=True，即不会更新词嵌入。\n",
    "\n",
    "#### **3.3 文本分类网络**\n",
    "\n",
    "请基于在上文给出的数据处理和词嵌入矩阵，完成以下文本分类代码。包括四个部分，定义文本分类网络，实现训练函数，实现测试函数以及定义损失函数。\n"
   ],
   "id": "815f532d5e8c2512"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#TODO:定义文本分类网络\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        #TODO: 实现模型结构\n",
    "        #TODO 实现self.embedding: 嵌入层\n",
    "        #TODO 实现self.fc: 分类层\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #TODO: 对一个句子中的所有单词的嵌入取平均得到最终的文档嵌入\n",
    "        return self.fc(x)\n",
    "\n",
    "# TODO: 实现训练函数，注意要把数据也放到gpu上避免报错\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "\n",
    "\n",
    "# TODO: 实现测试函数，返回在测试集上的准确率\n",
    "def evaluate_model(model, dataloader):\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_matrix = torch.Tensor(embedding_matrix)\n",
    "model = TextClassifier(embedding_matrix).to(device)\n",
    "#TODO 实现criterion: 定义交叉熵损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "    acc = evaluate_model(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}, Accuracy: {acc*100:.2f}%\")"
   ],
   "id": "33515ab832bc9315"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\"思考题\"\n",
    "\n",
    "    使用Glove词嵌入进行初始化，是否比随机初始化取得更好的效果？\n",
    "\n",
    "\"思考题\"\n",
    "\n",
    "    上述代码在不改变模型（即仍然只有self.embedding和self.fc，不额外引入如dropout等层）和超参数（即batch size和学习率）的情况下，我们可以修改哪些地方来提升模型性能。请列举两个方面。"
   ],
   "id": "cdc19b6ffcd03bc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
