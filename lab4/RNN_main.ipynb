{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6376419e",
   "metadata": {},
   "source": [
    "# å®éªŒä»»åŠ¡äºŒ: RNNã€LSTMå’ŒGRUæ–‡æœ¬ç”Ÿæˆä»»åŠ¡\n",
    "\n",
    "## **1. æ–‡æœ¬é¢„å¤„ç†**\n",
    "æ–‡æœ¬é¢„å¤„ç†ç®€ä»‹\n",
    "    æ–‡æœ¬é¢„å¤„ç†æ˜¯åœ¨æ·±åº¦å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ï¼Œå¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œæ¸…ç†ã€è½¬æ¢å’Œæ ¼å¼åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿè¢«æ¨¡å‹ç†è§£å’Œå¤„ç†çš„è¿‡ç¨‹ã€‚\n",
    "\n",
    "é¢„å¤„ç†çš„å¿…è¦æ€§\n",
    "    åŸå§‹æ–‡æœ¬å¯èƒ½åŒ…å«å™ªå£°ï¼Œä¸”æ–‡æœ¬é•¿åº¦ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ‰¹é‡è®­ç»ƒæ—¶éœ€è¦å¡«å……\n",
    "\n",
    "AG News æ•°æ®é›†ç®€ä»‹\n",
    "\n",
    "    AG News æ•°æ®é›†æ¥æºäº AG's corpus of news articlesï¼Œæ˜¯ä¸€ä¸ªå¤§å‹çš„æ–°é—»æ•°æ®é›†ï¼Œç”± Antonio Gulli ä»å¤šä¸ªæ–°é—»ç½‘ç«™æ”¶é›†æ•´ç†ã€‚\n",
    "    AG News æ•°æ®é›†åŒ…å« 4 ç±»æ–°é—»ï¼Œæ¯ç±» 30,000 æ¡è®­ç»ƒæ•°æ®ï¼Œå…± 120,000 æ¡è®­ç»ƒæ ·æœ¬ å’Œ 7,600 æ¡æµ‹è¯•æ ·æœ¬ã€‚\n",
    "\n",
    "é¦–å…ˆå¯¼å…¥æ‰€éœ€æ¨¡å—ï¼š\n",
    "\n",
    "å¯èƒ½éœ€è¦å®‰è£…datasetsåŒ…\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install datasets",
   "id": "f870a68277a4854a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "id": "17399ab3db54117"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "æˆ‘ä»¬ä»AG News æ•°æ®é›†ä¸­åŠ è½½æ–‡æœ¬ã€‚ è¿™æ˜¯ä¸€ä¸ªè¾ƒå°çš„è¯­æ–™åº“ï¼Œæœ‰150000å¤šä¸ªå•è¯ï¼Œä½†è¶³å¤Ÿæˆ‘ä»¬å°è¯•ç‰›åˆ€.\n",
   "id": "7c858707e89b9c13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path = \"ag_newsæ–‡ä»¶å¤¹ä¿å­˜è·¯å¾„\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# æå–æ‰€æœ‰æ–‡æœ¬æ•°æ®\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "caf5d0a68732b84d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "è¯å…ƒåŒ–\n",
    "ä¸‹é¢çš„tokenizeå‡½æ•°å°†æ–‡æœ¬è¡Œåˆ—è¡¨ï¼ˆlinesï¼‰ä½œä¸ºè¾“å…¥ï¼Œ åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼ˆå¦‚ä¸€æ¡æ–‡æœ¬è¡Œï¼‰ã€‚ æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªè¯å…ƒåˆ—è¡¨ï¼Œè¯å…ƒï¼ˆtokenï¼‰æ˜¯æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚ æœ€åï¼Œè¿”å›ä¸€ä¸ªç”±è¯å…ƒåˆ—è¡¨ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯å…ƒéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆstringï¼‰ã€‚\n"
   ],
   "id": "78ab1a668ad3d165"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ä½¿ç”¨ split è¿›è¡Œåˆ†è¯\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# ç”Ÿæˆè¯æ±‡è¡¨\n",
    "counter = Counter()\n",
    "for text in train_text:\n",
    "    counter.update(tokenize(text))"
   ],
   "id": "2748b26c2e6c70f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "è¯å…ƒçš„ç±»å‹æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œæ¨¡å‹éœ€è¦çš„è¾“å…¥æ˜¯æ•°å­—ï¼Œå› æ­¤è¿™ç§ç±»å‹ä¸æ–¹ä¾¿æ¨¡å‹ä½¿ç”¨ã€‚ ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå­—å…¸ï¼Œé€šå¸¸ä¹Ÿå«åšè¯è¡¨ï¼ˆvocabularyï¼‰ï¼Œ ç”¨æ¥å°†å­—ç¬¦ä¸²ç±»å‹çš„è¯å…ƒæ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­ã€‚\n",
    "é¦–å…ˆï¼Œå®šä¹‰ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ <unk> ä»£è¡¨æœªçŸ¥è¯ï¼Œ<pad> ç”¨äºåºåˆ—å¡«å……ï¼Œ<bos>è¡¨ç¤ºåºåˆ—å¼€å§‹ï¼Œ<eos>è¡¨ç¤ºåºåˆ—ç»“æŸï¼‰ã€‚ç„¶åï¼Œä» Counter ç»Ÿè®¡çš„å•è¯é¢‘ç‡åˆ—è¡¨ä¸­æå–æ‰€æœ‰å•è¯ï¼Œå¹¶æŒ‰é¢‘ç‡æ’åºï¼Œå°†å…¶æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚æœ€åï¼Œä½¿ç”¨ enumerate ä¸ºæ¯ä¸ªå•è¯åˆ†é…å”¯ä¸€ç´¢å¼•ï¼Œåˆ›å»ºä¸€ä¸ª word-to-index æ˜ å°„ï¼Œæ–¹ä¾¿å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼åºåˆ—ä¾›æ·±åº¦å­¦ä¹ æ¨¡å‹ä½¿ç”¨ã€‚\n"
   ],
   "id": "b1f7962eff64eb92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ç”Ÿæˆè¯æ±‡è¡¨ï¼ŒåŒ…å«ç‰¹æ®Š token\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = special_tokens + [word for word, _ in counter.most_common()]\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}"
   ],
   "id": "e4beb7d758643a55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "æ‰“å°è¯æ±‡è¡¨å¤§å°ï¼Œå‰10ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•ã€‚"
   ],
   "id": "a53a0504ff359aa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"è¯æ±‡è¡¨å¤§å°:\", len(vocab_dict))\n",
    "print(\"å‰ 10 ä¸ªæœ€å¸¸è§çš„å•è¯åŠå…¶ç´¢å¼•:\")\n",
    "#TODO:æ‰“å°å‰10ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•"
   ],
   "id": "161c78fff5d25175"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "    æ€è€ƒé¢˜1ï¼šåœ¨æ–‡æœ¬å¤„ç†ä¸­ï¼Œä¸ºä»€ä¹ˆéœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼ˆTokenizationï¼‰ï¼Ÿ\n",
    "\n",
    "    æ€è€ƒé¢˜2ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ä½¿ç”¨å•è¯è€Œéœ€è¦å°†å…¶è½¬æ¢ä¸ºç´¢å¼•ï¼Ÿ"
   ],
   "id": "b734720ce2043c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2. RNNæ–‡æœ¬ç”Ÿæˆå®éªŒ**\n",
    "\n",
    "\"RNNæ–‡æœ¬ç”Ÿæˆæ¦‚è¿°\"\n",
    "\n",
    "    ä½¿ç”¨RNNè¿›è¡Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„æ ¸å¿ƒæ€æƒ³æ˜¯ æ ¹æ®å‰é¢çš„æ–‡æœ¬é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œç„¶åå°†é¢„æµ‹å‡ºçš„å•è¯ä½œä¸ºè¾“å…¥ï¼Œå¾ªç¯è¿­ä»£ç”Ÿæˆå®Œæ•´æ–‡æœ¬ã€‚æœ¬å®éªŒä»¥AG News æ•°æ®ä¸ºä¾‹ï¼Œç»™å®šå‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚\n",
    "\n",
    "\"RNNçš„å±€é™æ€§\"\n",
    "\n",
    "    RNNçš„å±€é™æ€§åœ¨äºéš¾ä»¥è®°ä½é•¿è·ç¦»ä¸Šä¸‹æ–‡ï¼Œå®¹æ˜“å¯¼è‡´ç”Ÿæˆå†…å®¹ç¼ºä¹è¿è´¯æ€§ï¼Œä¸”å¯èƒ½å‡ºç°é‡å¤æˆ–æ¨¡å¼åŒ–çš„æ–‡æœ¬ã€‚\n",
    "\n",
    "![ç¤ºä¾‹å›¾ç‰‡](pics/rnn.png)\n",
    "\n",
    "### å‰ç½®ä»£ç \n",
    "\n",
    "é¦–å…ˆå¯¼å…¥æ‰€éœ€æ¨¡å—ï¼š"
   ],
   "id": "f4fc14a970d1d30f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "id": "910d8e3c07f67072"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "è¯»å–æ•°æ®é›†",
   "id": "9e5dfbf26750f226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path = \"ag_newsæ–‡ä»¶å¤¹ä¿å­˜è·¯å¾„\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# æå–æ‰€æœ‰æ–‡æœ¬æ•°æ®\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "8717c9e95d9e8e58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "æ–‡æœ¬çš„é¢„å¤„ç†",
   "id": "80d041e69fb63a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ä½¿ç”¨ split è¿›è¡Œåˆ†è¯\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# ç”Ÿæˆè¯æ±‡è¡¨\n",
    "counter = Counter()\n",
    "for text in train_text:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "# ç”Ÿæˆè¯æ±‡è¡¨ï¼ŒåŒ…å«ç‰¹æ®Š token\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = special_tokens + [word for word, _ in counter.most_common()]\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n"
   ],
   "id": "ea3afddf94ced251"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### è®­ç»ƒæ•°æ®ç”Ÿæˆ\n",
    "\n",
    "å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œå¹¶æŒ‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ã€ä¸‹ä¸€ä¸ªå•è¯ä½œä¸ºç›®æ ‡çš„æ–¹å¼æ„é€ è®­ç»ƒæ•°æ®ã€‚æœ€ç»ˆç”Ÿæˆ X_trainï¼ˆè¾“å…¥åºåˆ—ï¼‰å’Œ Y_trainï¼ˆé¢„æµ‹ç›®æ ‡ï¼‰ï¼Œç”¨äº RNN è®­ç»ƒæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚\n"
   ],
   "id": "cf8d9b04dd0c2f9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def numericalize(text):\n",
    "    return torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "# ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆè¾“å…¥ 100 ä¸ªè¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰\n",
    "def create_data(text_list, seq_len=100):\n",
    "    X, Y = [], []\n",
    "    for text in text_list:\n",
    "        token_ids = numericalize(text)\n",
    "        if len(token_ids) <= seq_len:\n",
    "            continue  # å¿½ç•¥è¿‡çŸ­çš„æ–‡æœ¬\n",
    "        for i in range(len(token_ids) - seq_len):\n",
    "            X.append(token_ids[i:i + seq_len])\n",
    "            Y.append(token_ids[i + seq_len])\n",
    "    return torch.stack(X), torch.tensor(Y)\n",
    "\n",
    "# ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "X_train, Y_train = create_data(train_text, seq_len=100)\n",
    "\n",
    "\n",
    "# åˆ›å»º DataLoader\n",
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ],
   "id": "d33c3dfdd9212f39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "    æ€è€ƒé¢˜3ï¼šå¦‚æœä¸æ‰“ä¹±è®­ç»ƒé›†ï¼Œä¼šå¯¹ç”Ÿæˆä»»åŠ¡æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
    "\n",
    "\n",
    "### RNN æ¨¡å‹æ„å»º\n",
    "\n",
    "å®ç°äº†ä¸€ä¸ªåŸºäº RNN çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚"
   ],
   "id": "f47ecbe3b4987b16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RNNTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)#å°†è¾“å…¥çš„å•è¯ç´¢å¼•è½¬æ¢ä¸º embed_dim ç»´çš„å‘é‡ã€‚\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)#æ„å»ºä¸€ä¸ª RNN å±‚ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®ã€‚\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)#å°† RNN éšè—çŠ¶æ€ æ˜ å°„åˆ° è¯æ±‡è¡¨å¤§å°çš„å‘é‡ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        #è¾“å…¥ x å½¢çŠ¶ï¼š(batch_size, seq_len)\n",
    "        #è¾“å‡º embedded å½¢çŠ¶ï¼š(batch_size, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        #è¾“å…¥ embedded å½¢çŠ¶ï¼š(batch_size, seq_len, embed_dim)\n",
    "        #è¾“å‡º output å½¢çŠ¶ï¼š(batch_size, seq_len, hidden_dim)ï¼ˆæ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰\n",
    "        #è¾“å‡º hidden å½¢çŠ¶ï¼š(num_layers, batch_size, hidden_dim)ï¼ˆæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        #åªå– æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ output[:, -1, :] ä½œä¸ºè¾“å…¥\n",
    "        #é€šè¿‡å…¨è¿æ¥å±‚ self.fc å°†éšè—çŠ¶æ€è½¬æ¢ä¸ºè¯æ±‡è¡¨å¤§å°çš„åˆ†å¸ƒï¼ˆç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼‰\n",
    "        #æœ€ç»ˆ output å½¢çŠ¶ï¼š(batch_size, vocab_size)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden"
   ],
   "id": "147d6c280095cc9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "å®šä¹‰æ¨¡å‹æ‰€éœ€å‚æ•°ã€å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨",
   "id": "d5f0eae4db36a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = RNNTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "c7b124a501affec7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RNN æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "RNN è®­ç»ƒè¿‡ç¨‹"
   ],
   "id": "83f9e86d4297477c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    model.train()# å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")# ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡\n",
    "        epoch_grad_norm = None\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)# å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰\n",
    "            optimizer.zero_grad()# æ¸…ç©ºä¸Šä¸€è½®çš„æ¢¯åº¦ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯\n",
    "\n",
    "            output, _ = model(X_batch)# å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æ¨¡å‹è¾“å‡º\n",
    "            loss = criterion(output, Y_batch) # è®¡ç®—æŸå¤±å‡½æ•°å€¼\n",
    "            loss.backward()# åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦\n",
    "\n",
    "            optimizer.step() # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "            total_loss += loss.item()# ç´¯åŠ å½“å‰ batch çš„æŸå¤±å€¼\n",
    "            progress_bar.set_postfix(loss=loss.item())# åœ¨è¿›åº¦æ¡ä¸Šæ˜¾ç¤ºå½“å‰ batch çš„æŸå¤±å€¼\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        # è®¡ç®—å¹¶è¾“å‡ºæœ¬è½®è®­ç»ƒçš„å¹³å‡æŸå¤±\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "train_model(model, train_loader, epochs=20)"
   ],
   "id": "3e3a4b5e87d77a12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RNN æ¨¡å‹æµ‹è¯•\n",
    "\n",
    "RNN ç”Ÿæˆæ–‡æœ¬æµ‹è¯•"
   ],
   "id": "4b633f9741598290"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()# å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨ dropout å’Œ batch normalization\n",
    "    words = tokenize(start_text)# å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè·å–åˆå§‹è¯åˆ—è¡¨\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œå¹¶è°ƒæ•´å½¢çŠ¶ä»¥ç¬¦åˆæ¨¡å‹è¾“å…¥æ ¼å¼ï¼ˆå¢åŠ  batch ç»´åº¦ï¼‰ï¼Œå†ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words): # ç”Ÿæˆ num_words ä¸ªå•è¯\n",
    "        with torch.no_grad(): # åœ¨æ¨ç†æ—¶å…³é—­æ¢¯åº¦è®¡ç®—ï¼Œæé«˜æ•ˆç‡\n",
    "            output, hidden = model(input_seq, hidden)# å‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹è¾“å‡ºå’Œæ–°çš„éšè—çŠ¶æ€\n",
    "\n",
    "        # è®¡ç®— softmaxï¼Œå¹¶åº”ç”¨æ¸©åº¦ç³»æ•°\n",
    "        logits = output.squeeze(0) / temperature # å¯¹ logits é™¤ä»¥ temperature è°ƒèŠ‚æ¦‚ç‡åˆ†å¸ƒçš„å¹³æ»‘åº¦\n",
    "        probs = F.softmax(logits, dim=-1) # è®¡ç®— softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ\n",
    "\n",
    "        # é‡‡æ ·æ–°è¯\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        # åŸºäºæ¦‚ç‡åˆ†å¸ƒ éšæœºé‡‡æ ·ä¸€ä¸ªè¯çš„ç´¢å¼•\n",
    "\n",
    "        next_word = vocab[predicted_id]  # ä»è¯è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„å•è¯\n",
    "        words.append(next_word)# å°†ç”Ÿæˆçš„å•è¯æ·»åŠ åˆ°æ–‡æœ¬åˆ—è¡¨ä¸­\n",
    "\n",
    "        # æ›´æ–°è¾“å…¥åºåˆ—ï¼ˆå°†æ–°è¯åŠ å…¥ï¼Œå¹¶ç§»é™¤æœ€æ—§çš„è¯ï¼Œç»´æŒè¾“å…¥é•¿åº¦ï¼‰\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# å–å‰ 100 ä¸ªå•è¯ä½œä¸ºå‰ç¼€\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# è®©æ¨¡å‹åŸºäºè¯¥å‰ç¼€ç”Ÿæˆ 100 ä¸ªè¯\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "\n",
    "print(\"\\nğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š\\n\")\n",
    "print(generated_text)"
   ],
   "id": "97bbe30b05eb568"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### å›°æƒ‘åº¦è¯„ä¼°\n",
    "\n",
    "**1. åŸºæœ¬æ¦‚å¿µ**\n",
    "å›°æƒ‘åº¦ï¼ˆPerplexity, PPLï¼‰æ˜¯è¡¡é‡è¯­è¨€æ¨¡å‹å¥½åçš„ä¸€ä¸ªå¸¸è§æŒ‡æ ‡ï¼Œå®ƒè¡¨ç¤ºæ¨¡å‹å¯¹æµ‹è¯•æ•°æ®çš„ä¸ç¡®å®šæ€§ï¼Œå³æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶çš„å›°æƒ‘ç¨‹åº¦ã€‚\n",
    "å¦‚æœä¸€ä¸ªæ¨¡å‹çš„å›°æƒ‘åº¦è¶Šä½ï¼Œè¯´æ˜å®ƒå¯¹æ•°æ®çš„é¢„æµ‹è¶Šå‡†ç¡®ï¼Œå³æ›´â€œç¡®ä¿¡â€è‡ªå·±ç”Ÿæˆçš„è¯è¯­ï¼›å¦‚æœå›°æƒ‘åº¦é«˜ï¼Œè¯´æ˜æ¨¡å‹çš„é¢„æµ‹ä¸å¤ªç¡®å®šï¼Œå¯èƒ½åœ¨å¤šä¸ªè¯ä¹‹é—´æ‘‡æ‘†ä¸å®šã€‚\n",
    "\n",
    "**2. æ•°å­¦å®šä¹‰**\n",
    "\n",
    "å‡è®¾ä¸€ä¸ªå¥å­ç”±$N$ä¸ªå•è¯ç»„æˆï¼š\n",
    "\n",
    "$$W=(w_1,w_2,...,w_N)L_{total}(\\mathbf{w}, b) = L_{original}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "æ¨¡å‹ç»™å‡ºçš„æ¦‚ç‡ä¸ºï¼š\n",
    "\n",
    "$$P(W)=P(w_1,w_2,...,w_N)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_N|w_1,...,w_{N-1})$$\n",
    "\n",
    "é‚£ä¹ˆï¼Œå›°æƒ‘åº¦ï¼ˆPerplexity, PPLï¼‰å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "PPL=P(W)^{-\\frac{1}{N}}\n",
    "$$\n",
    "\n",
    "æˆ–è€…ç­‰ä»·åœ°ï¼š\n",
    "\n",
    "$$\n",
    "PPL = \\exp \\left( -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_1, ..., w_{i-1}) \\right)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $P(w_i | w_1, ..., w_{i-1})$ æ˜¯æ¨¡å‹åœ¨ç»™å®šå‰ $i-1$ ä¸ªå•è¯æ—¶é¢„æµ‹ $w_i$ çš„æ¦‚ç‡\n",
    "- $N$ æ˜¯å¥å­çš„å•è¯æ€»æ•°\n",
    "\n",
    "å›°æƒ‘åº¦çš„æœ€å¥½çš„ç†è§£æ˜¯â€œä¸‹ä¸€ä¸ªè¯å…ƒçš„å®é™…é€‰æ‹©æ•°çš„è°ƒå’Œå¹³å‡æ•°â€ã€‚\n",
    "\n",
    "- åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯å®Œç¾åœ°ä¼°è®¡æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º1ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º1ã€‚\n",
    "\n",
    "- åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯é¢„æµ‹æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º0ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦æ˜¯æ­£æ— ç©·å¤§ã€‚\n",
    "\n",
    "ä¸‹é¢è¯·ä½ æŒ‰ç…§è¦æ±‚è¡¥å…¨è®¡ç®—å›°æƒ‘åº¦çš„ä»£ç "
   ],
   "id": "b07eed60a7330957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_perplexity(model, test_text, vocab_dict, seq_len=100):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç»™å®šæ–‡æœ¬çš„å›°æƒ‘åº¦ï¼ˆPerplexity, PPLï¼‰\n",
    "\n",
    "    :param model: è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹ï¼ˆRNN/LSTMï¼‰\n",
    "    :param test_text: éœ€è¦è¯„ä¼°çš„æ–‡æœ¬\n",
    "    :param vocab_dict: è¯æ±‡è¡¨ï¼ˆç”¨äºè½¬æ¢æ–‡æœ¬åˆ°ç´¢å¼•ï¼‰\n",
    "    :param seq_len: è¯„ä¼°æ—¶çš„çª—å£å¤§å°\n",
    "    :return: PPL å›°æƒ‘åº¦\n",
    "    \"\"\"\n",
    "    model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    words = test_text.lower().split()\n",
    "\n",
    "    # å°†æ–‡æœ¬è½¬æ¢ä¸º token IDï¼Œå¦‚æœè¯ä¸åœ¨è¯è¡¨ä¸­ï¼Œåˆ™ä½¿ç”¨ \"<unk>\"ï¼ˆæœªçŸ¥è¯ï¼‰å¯¹åº”çš„ç´¢å¼•\n",
    "    token_ids = torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in words], dtype=torch.long)\n",
    "\n",
    "    # è®¡ç®— PPL\n",
    "    total_log_prob = 0\n",
    "    num_tokens = len(token_ids) - 1  # é¢„æµ‹ num_tokens æ¬¡\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_tokens):\n",
    "            \"\"\"éå†æ–‡æœ¬çš„æ¯ä¸ª tokenï¼Œè®¡ç®—å…¶æ¡ä»¶æ¦‚ç‡ï¼Œæœ€åç´¯åŠ logæ¦‚ç‡\"\"\"\n",
    "            input_seq = token_ids[max(0, i - seq_len):i].unsqueeze(0).to(device)  # è·å–å‰ seq_len ä¸ªå•è¯\n",
    "            if input_seq.shape[1] == 0:  # é¿å… RNN è¾“å…¥ç©ºåºåˆ—\n",
    "                continue\n",
    "\n",
    "            target_word = token_ids[i].unsqueeze(0).to(device)  # ç›®æ ‡å•è¯\n",
    "\n",
    "            # TODO: å‰å‘ä¼ æ’­ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„ logits\n",
    "            # TODO: è®¡ç®— softmax å¹¶å– log æ¦‚ç‡\n",
    "            # TODO: å–ç›®æ ‡è¯çš„å¯¹æ•°æ¦‚ç‡\n",
    "            # TODO: ç´¯åŠ  log æ¦‚ç‡\n",
    "\n",
    "    avg_log_prob = total_log_prob / num_tokens  # è®¡ç®—å¹³å‡ log æ¦‚ç‡\n",
    "    perplexity = torch.exp(torch.tensor(-avg_log_prob)) # è®¡ç®— PPLï¼Œå…¬å¼ PPL = exp(-avg_log_prob)\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "ppl = compute_perplexity(model, generated_text, vocab_dict)\n",
    "print(f\"Perplexity (PPL): {ppl:.4f}\")"
   ],
   "id": "8128c21518c1c396"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "    æ€è€ƒé¢˜4ï¼šå‡è®¾ä½ åœ¨RNNå’ŒLSTMè¯­è¨€æ¨¡å‹ä¸Šåˆ†åˆ«è®¡ç®—äº†å›°æƒ‘åº¦ï¼Œå‘ç°RNNçš„PPLæ›´ä½ã€‚è¿™æ˜¯å¦æ„å‘³ç€RNNç”Ÿæˆçš„æ–‡æœ¬ä¸€å®šæ›´æµç•…è‡ªç„¶ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œåœ¨ä»€ä¹ˆæƒ…å†µä¸‹è¿™ä¸¤ä¸ªå›°æƒ‘åº¦å¯ä»¥ç›´æ¥æ¯”è¾ƒï¼Ÿ\n",
    "\n",
    "    æ€è€ƒé¢˜5ï¼šå›°æƒ‘åº¦æ˜¯ä¸æ˜¯è¶Šä½è¶Šå¥½ï¼Ÿ\n",
    "\n",
    "\n",
    "## **3. LSTMå’ŒGRUæ–‡æœ¬ç”Ÿæˆå®éªŒ**\n",
    "\n",
    "LSTMæ–‡æœ¬ç”Ÿæˆæ¦‚è¿°\n",
    "\n",
    "    LSTMï¼ˆLong Short-Term Memoryï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„ RNNï¼Œèƒ½å¤Ÿé€šè¿‡ é—¨æ§æœºåˆ¶ï¼ˆé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨ï¼‰ æœ‰æ•ˆæ•æ‰é•¿æœŸä¾èµ–å…³ç³»ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä½¿å…¶åœ¨å¤„ç†é•¿åºåˆ—ä»»åŠ¡æ—¶æ¯”æ™®é€š RNN æ›´å¼ºå¤§ã€‚\n",
    "    æœ¬å®éªŒä¾æ—§ä»¥AG News æ•°æ®ä¸ºä¾‹ï¼Œç»™å®šå‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚\n",
    "\n",
    "\n",
    "![ç¤ºä¾‹å›¾ç‰‡](pics/lstm.png)\n",
    "\n",
    "æ–‡æœ¬çš„é¢„å¤„ç† è®­ç»ƒæ•°æ®ç”Ÿæˆä¸å‰é¢ä¸€è‡´\n",
    "\n",
    "\n",
    "### LSTM æ¨¡å‹æ„å»º\n",
    "\n",
    "å®ç°äº†ä¸€ä¸ªåŸºäº LSTM çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚"
   ],
   "id": "b8a7b17b77b24641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LSTMTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(LSTMTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)  # (B, L, embed_dim)\n",
    "        output, hidden = self.lstm(embedded, hidden)  # (B, L, hidden_dim)\n",
    "        output = self.fc(output[:, -1, :])  # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œé¢„æµ‹\n",
    "        return output, hidden"
   ],
   "id": "59e99eafebcf8efa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "å®šä¹‰æ¨¡å‹æ‰€éœ€å‚æ•°ã€å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨",
   "id": "6b4888c5788f06a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = LSTMTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "5b82d0a3e6c651a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LSTM æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "LSTM è®­ç»ƒè¿‡ç¨‹"
   ],
   "id": "8431d89227157ead"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        epoch_grad_norm = None\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "train_model(model, train_loader, epochs=20)"
   ],
   "id": "7a45bac2899ca925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LSTM æ¨¡å‹æµ‹è¯•\n",
    "\n",
    "LSTM ç”Ÿæˆæ–‡æœ¬æµ‹è¯•"
   ],
   "id": "8a4eaf82728ce550"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = tokenize(start_text)\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "\n",
    "        # è®¡ç®— softmaxï¼Œå¹¶åº”ç”¨æ¸©åº¦ç³»æ•°\n",
    "        logits = output.squeeze(0) / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # é‡‡æ ·æ–°è¯\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        next_word = vocab[predicted_id]\n",
    "        words.append(next_word)\n",
    "\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# å–å‰ 100 ä¸ªå•è¯ä½œä¸ºå‰ç¼€\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# è®©æ¨¡å‹åŸºäºè¯¥å‰ç¼€ç”Ÿæˆ 100 ä¸ªè¯\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "print(\"\\nğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š\\n\")\n",
    "print(generated_text)"
   ],
   "id": "9d1b23f6cbedfc4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "å€ŸåŠ©RNNæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è®¡ç®—å›°æƒ‘åº¦çš„å‡½æ•°ï¼Œè®¡ç®—ä¸€ä¸‹lstmåœ¨generated_textä¸Šçš„å›°æƒ‘åº¦ã€‚\n",
    "\n",
    "\n",
    "    æ€è€ƒé¢˜6ï¼šè§‚å¯Ÿä¸€ä¸‹RNNå’ŒLSTMè®­ç»ƒè¿‡ç¨‹ä¸­lossçš„å˜åŒ–ï¼Œå¹¶åˆ†æä¸€ä¸‹é€ æˆè¿™ç§ç°è±¡çš„åŸå› ã€‚\n",
    "\n",
    "\n",
    "\n",
    "GRUæ–‡æœ¬ç”Ÿæˆæ¦‚è¿°\n",
    "\n",
    "    GRUï¼ˆGated Recurrent Unitï¼‰æ˜¯ LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ æ›´æ–°é—¨ï¼ˆUpdate Gateï¼‰å’Œé‡ç½®é—¨ï¼ˆReset Gateï¼‰ æ¥æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œä¸”èƒ½åœ¨è®¸å¤šä»»åŠ¡ä¸­å–å¾—ä¸ LSTM ç›¸ä¼¼çš„æ•ˆæœï¼ŒåŒæ—¶å‡å°‘è®¡ç®—æˆæœ¬å’Œå‚æ•°é‡ã€‚\n",
    "    æœ¬å®éªŒä¾æ—§ä»¥AG News æ•°æ®ä¸ºä¾‹ï¼Œç»™å®šå‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚\n",
    "\n",
    "\n",
    "![ç¤ºä¾‹å›¾ç‰‡](pics/GRU.png)\n",
    "\n",
    "\n",
    "æ–‡æœ¬çš„é¢„å¤„ç† è®­ç»ƒæ•°æ®ç”Ÿæˆä¸å‰é¢ä¸€è‡´\n",
    "\n",
    "\n",
    "### GRU æ¨¡å‹æ„å»º\n",
    "\n",
    "å®ç°äº†ä¸€ä¸ªåŸºäº GRU çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚"
   ],
   "id": "93f1f41e00b2551a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GRUTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(GRUTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)  # (B, L, embed_dim)\n",
    "        output, hidden = self.gru(embedded, hidden)  # (B, L, hidden_dim)\n",
    "        output = self.fc(output[:, -1, :])  # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œé¢„æµ‹\n",
    "        return output, hidden"
   ],
   "id": "14ee692edfce0cd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "å®šä¹‰æ¨¡å‹æ‰€éœ€å‚æ•°ã€å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨",
   "id": "3c265257cd9565af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = GRUTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "d59bd931bf15b6c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GRU æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "GRU è®­ç»ƒè¿‡ç¨‹ä¹Ÿä¸LSTMä¿æŒä¸€è‡´\n",
    "\n",
    "\n",
    "### GRU æ¨¡å‹æµ‹è¯•\n",
    "\n",
    "GRU ç”Ÿæˆæ–‡æœ¬æµ‹è¯•"
   ],
   "id": "73ad0ad957f31fa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = tokenize(start_text)\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "\n",
    "        # è®¡ç®— softmaxï¼Œå¹¶åº”ç”¨æ¸©åº¦ç³»æ•°\n",
    "        logits = output.squeeze(0) / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # é‡‡æ ·æ–°è¯\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        next_word = vocab[predicted_id]\n",
    "        words.append(next_word)\n",
    "\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# å–å‰ 100 ä¸ªå•è¯ä½œä¸ºå‰ç¼€\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# è®©æ¨¡å‹åŸºäºè¯¥å‰ç¼€ç”Ÿæˆ 100 ä¸ªè¯\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "print(\"\\nğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š\\n\")\n",
    "print(generated_text)"
   ],
   "id": "c79300a967f2fd4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "å€ŸåŠ©RNNæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è®¡ç®—å›°æƒ‘çš„å‡½æ•°ï¼Œè®¡ç®—ä¸€ä¸‹GRUåœ¨generated_textä¸Šçš„å›°æƒ‘åº¦ã€‚\n",
    "\n",
    "\n",
    "    æ€è€ƒé¢˜7ï¼šè¿™ä¸‰ä¸ªå›°æƒ‘åº¦å¯ä»¥ç›´æ¥æ¯”è¾ƒå—ï¼Ÿåˆ†æä¸€ä¸‹ã€‚\n",
    "\n",
    "    æ€è€ƒé¢˜8ï¼šGRU åªæœ‰ä¸¤ä¸ªé—¨ï¼ˆæ›´æ–°é—¨å’Œé‡ç½®é—¨ï¼‰ï¼Œç›¸æ¯” LSTM å°‘äº†ä¸€ä¸ªé—¨æ§å•å…ƒï¼Œè¿™æ ·çš„è®¾è®¡æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ\n",
    "\n",
    "    æ€è€ƒé¢˜9ï¼šåœ¨ä½ç®—åŠ›è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºï¼‰ä¸Šï¼ŒRNNã€LSTM å’Œ GRU å“ªä¸ªæ›´é€‚åˆéƒ¨ç½²ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "    æ€è€ƒé¢˜10ï¼šå¦‚æœå°±æ˜¯è¦ä½¿ç”¨RNNæ¨¡å‹ï¼ŒåŸå…ˆçš„ä»£ç è¿˜æœ‰å“ªé‡Œå¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹ï¼Ÿè¯·ç»™å‡ºä¿®æ”¹éƒ¨åˆ†ä»¥åŠå®éªŒç»“æœã€‚"
   ],
   "id": "99a4dc9495339214"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
