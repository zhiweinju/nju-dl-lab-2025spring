# å®éªŒä»»åŠ¡äºŒ: RNNã€LSTMå’ŒGRUæ–‡æœ¬ç”Ÿæˆä»»åŠ¡

## **1. æ–‡æœ¬é¢„å¤„ç†**

!!! info "æ–‡æœ¬é¢„å¤„ç†ç®€ä»‹"
    æ–‡æœ¬é¢„å¤„ç†æ˜¯åœ¨æ·±åº¦å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ï¼Œå¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œæ¸…ç†ã€è½¬æ¢å’Œæ ¼å¼åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿè¢«æ¨¡å‹ç†è§£å’Œå¤„ç†çš„è¿‡ç¨‹ã€‚

!!! warning "é¢„å¤„ç†çš„å¿…è¦æ€§"
    åŸå§‹æ–‡æœ¬å¯èƒ½åŒ…å«å™ªå£°ï¼Œä¸”æ–‡æœ¬é•¿åº¦ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ‰¹é‡è®­ç»ƒæ—¶éœ€è¦å¡«å……

!!! info "AG News æ•°æ®é›†ç®€ä»‹"
    AG News æ•°æ®é›†æ¥æºäº AG's corpus of news articlesï¼Œæ˜¯ä¸€ä¸ªå¤§å‹çš„æ–°é—»æ•°æ®é›†ï¼Œç”± Antonio Gulli ä»å¤šä¸ªæ–°é—»ç½‘ç«™æ”¶é›†æ•´ç†ã€‚
    AG News æ•°æ®é›†åŒ…å« 4 ç±»æ–°é—»ï¼Œæ¯ç±» 30,000 æ¡è®­ç»ƒæ•°æ®ï¼Œå…± 120,000 æ¡è®­ç»ƒæ ·æœ¬ å’Œ 7,600 æ¡æµ‹è¯•æ ·æœ¬ã€‚

é¦–å…ˆå¯¼å…¥æ‰€éœ€æ¨¡å—ï¼š

!!! warning "å¯èƒ½éœ€è¦å…ˆå®‰è£…datasetsåŒ…"
```bash
   pip install datasets
```

```python
import torch
import torch.nn as nn
import torch.optim as optim
from datasets import load_dataset, load_from_disk
from collections import Counter
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from tqdm import tqdm
import os
```

æˆ‘ä»¬ä»AG News æ•°æ®é›†ä¸­åŠ è½½æ–‡æœ¬ã€‚ è¿™æ˜¯ä¸€ä¸ªè¾ƒå°çš„è¯­æ–™åº“ï¼Œæœ‰150000å¤šä¸ªå•è¯ï¼Œä½†è¶³å¤Ÿæˆ‘ä»¬å°è¯•ç‰›åˆ€.


```python
data_path = "ag_newsæ–‡ä»¶å¤¹ä¿å­˜è·¯å¾„"
dataset = load_from_disk(data_path)

# æå–æ‰€æœ‰æ–‡æœ¬æ•°æ®
train_text = [item['text'] for item in dataset['train']]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

è¯å…ƒåŒ–
ä¸‹é¢çš„tokenizeå‡½æ•°å°†æ–‡æœ¬è¡Œåˆ—è¡¨ï¼ˆlinesï¼‰ä½œä¸ºè¾“å…¥ï¼Œ åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼ˆå¦‚ä¸€æ¡æ–‡æœ¬è¡Œï¼‰ã€‚ æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªè¯å…ƒåˆ—è¡¨ï¼Œè¯å…ƒï¼ˆtokenï¼‰æ˜¯æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚ æœ€åï¼Œè¿”å›ä¸€ä¸ªç”±è¯å…ƒåˆ—è¡¨ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯å…ƒéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆstringï¼‰ã€‚


```python
# ä½¿ç”¨ split è¿›è¡Œåˆ†è¯
def tokenize(text):
    return text.lower().split()

# ç”Ÿæˆè¯æ±‡è¡¨
counter = Counter()
for text in train_text:
    counter.update(tokenize(text))
```

è¯å…ƒçš„ç±»å‹æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œæ¨¡å‹éœ€è¦çš„è¾“å…¥æ˜¯æ•°å­—ï¼Œå› æ­¤è¿™ç§ç±»å‹ä¸æ–¹ä¾¿æ¨¡å‹ä½¿ç”¨ã€‚ ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå­—å…¸ï¼Œé€šå¸¸ä¹Ÿå«åšè¯è¡¨ï¼ˆvocabularyï¼‰ï¼Œ ç”¨æ¥å°†å­—ç¬¦ä¸²ç±»å‹çš„è¯å…ƒæ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­ã€‚
é¦–å…ˆï¼Œå®šä¹‰ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ <unk> ä»£è¡¨æœªçŸ¥è¯ï¼Œ<pad> ç”¨äºåºåˆ—å¡«å……ï¼Œ<bos>è¡¨ç¤ºåºåˆ—å¼€å§‹ï¼Œ<eos>è¡¨ç¤ºåºåˆ—ç»“æŸï¼‰ã€‚ç„¶åï¼Œä» Counter ç»Ÿè®¡çš„å•è¯é¢‘ç‡åˆ—è¡¨ä¸­æå–æ‰€æœ‰å•è¯ï¼Œå¹¶æŒ‰é¢‘ç‡æ’åºï¼Œå°†å…¶æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚æœ€åï¼Œä½¿ç”¨ enumerate ä¸ºæ¯ä¸ªå•è¯åˆ†é…å”¯ä¸€ç´¢å¼•ï¼Œåˆ›å»ºä¸€ä¸ª word-to-index æ˜ å°„ï¼Œæ–¹ä¾¿å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼åºåˆ—ä¾›æ·±åº¦å­¦ä¹ æ¨¡å‹ä½¿ç”¨ã€‚

```python
# ç”Ÿæˆè¯æ±‡è¡¨ï¼ŒåŒ…å«ç‰¹æ®Š token
special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
vocab = special_tokens + [word for word, _ in counter.most_common()]
vocab_dict = {word: idx for idx, word in enumerate(vocab)}
```

æ‰“å°è¯æ±‡è¡¨å¤§å°ï¼Œå‰10ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•ã€‚


```python
print("è¯æ±‡è¡¨å¤§å°:", len(vocab_dict))
print("å‰ 10 ä¸ªæœ€å¸¸è§çš„å•è¯åŠå…¶ç´¢å¼•:")
#TODO:æ‰“å°å‰10ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•
```


!!! question "æ€è€ƒé¢˜"
    æ€è€ƒé¢˜1ï¼šåœ¨æ–‡æœ¬å¤„ç†ä¸­ï¼Œä¸ºä»€ä¹ˆéœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼ˆTokenizationï¼‰ï¼Ÿ

    æ€è€ƒé¢˜2ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ä½¿ç”¨å•è¯è€Œéœ€è¦å°†å…¶è½¬æ¢ä¸ºç´¢å¼•ï¼Ÿ


## **2. RNNæ–‡æœ¬ç”Ÿæˆå®éªŒ**

!!! abstract "RNNæ–‡æœ¬ç”Ÿæˆæ¦‚è¿°"
    ä½¿ç”¨RNNè¿›è¡Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„æ ¸å¿ƒæ€æƒ³æ˜¯ æ ¹æ®å‰é¢çš„æ–‡æœ¬é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œç„¶åå°†é¢„æµ‹å‡ºçš„å•è¯ä½œä¸ºè¾“å…¥ï¼Œå¾ªç¯è¿­ä»£ç”Ÿæˆå®Œæ•´æ–‡æœ¬ã€‚æœ¬å®éªŒä»¥AG News æ•°æ®ä¸ºä¾‹ï¼Œç»™å®šå‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚

!!! warning "RNNçš„å±€é™æ€§"
    RNNçš„å±€é™æ€§åœ¨äºéš¾ä»¥è®°ä½é•¿è·ç¦»ä¸Šä¸‹æ–‡ï¼Œå®¹æ˜“å¯¼è‡´ç”Ÿæˆå†…å®¹ç¼ºä¹è¿è´¯æ€§ï¼Œä¸”å¯èƒ½å‡ºç°é‡å¤æˆ–æ¨¡å¼åŒ–çš„æ–‡æœ¬ã€‚


![ç¤ºä¾‹å›¾ç‰‡](pics/rnn.png)

### å‰ç½®ä»£ç 

é¦–å…ˆå¯¼å…¥æ‰€éœ€æ¨¡å—ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from datasets import load_dataset, load_from_disk
from collections import Counter
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from tqdm import tqdm
import os
```

è¯»å–æ•°æ®é›†

```python
data_path = "ag_newsæ–‡ä»¶å¤¹ä¿å­˜è·¯å¾„"
dataset = load_from_disk(data_path)

# æå–æ‰€æœ‰æ–‡æœ¬æ•°æ®
train_text = [item['text'] for item in dataset['train']]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```


æ–‡æœ¬çš„é¢„å¤„ç†


```python

# ä½¿ç”¨ split è¿›è¡Œåˆ†è¯
def tokenize(text):
    return text.lower().split()

# ç”Ÿæˆè¯æ±‡è¡¨
counter = Counter()
for text in train_text:
    counter.update(tokenize(text))

# ç”Ÿæˆè¯æ±‡è¡¨ï¼ŒåŒ…å«ç‰¹æ®Š token
special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
vocab = special_tokens + [word for word, _ in counter.most_common()]
vocab_dict = {word: idx for idx, word in enumerate(vocab)}

```


### è®­ç»ƒæ•°æ®ç”Ÿæˆ

å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œå¹¶æŒ‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ã€ä¸‹ä¸€ä¸ªå•è¯ä½œä¸ºç›®æ ‡çš„æ–¹å¼æ„é€ è®­ç»ƒæ•°æ®ã€‚æœ€ç»ˆç”Ÿæˆ X_trainï¼ˆè¾“å…¥åºåˆ—ï¼‰å’Œ Y_trainï¼ˆé¢„æµ‹ç›®æ ‡ï¼‰ï¼Œç”¨äº RNN è®­ç»ƒæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚

```python

def numericalize(text):
    return torch.tensor([vocab_dict.get(word, vocab_dict["<unk>"]) for word in tokenize(text)], dtype=torch.long)

# ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆè¾“å…¥ 100 ä¸ªè¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰
def create_data(text_list, seq_len=100):
    X, Y = [], []
    for text in text_list:
        token_ids = numericalize(text)
        if len(token_ids) <= seq_len:
            continue  # å¿½ç•¥è¿‡çŸ­çš„æ–‡æœ¬
        for i in range(len(token_ids) - seq_len):
            X.append(token_ids[i:i + seq_len])
            Y.append(token_ids[i + seq_len])
    return torch.stack(X), torch.tensor(Y)

# ç”Ÿæˆè®­ç»ƒæ•°æ®
X_train, Y_train = create_data(train_text, seq_len=100)


# åˆ›å»º DataLoader
batch_size = 32
train_data = torch.utils.data.TensorDataset(X_train, Y_train)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)

```

!!! question "æ€è€ƒé¢˜"
    æ€è€ƒé¢˜3ï¼šå¦‚æœä¸æ‰“ä¹±è®­ç»ƒé›†ï¼Œä¼šå¯¹ç”Ÿæˆä»»åŠ¡æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ


### RNN æ¨¡å‹æ„å»º

å®ç°äº†ä¸€ä¸ªåŸºäº RNN çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚

```python
class RNNTextGenerator(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):
        super(RNNTextGenerator, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)#å°†è¾“å…¥çš„å•è¯ç´¢å¼•è½¬æ¢ä¸º embed_dim ç»´çš„å‘é‡ã€‚
        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)#æ„å»ºä¸€ä¸ª RNN å±‚ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®ã€‚
        self.fc = nn.Linear(hidden_dim, vocab_size)#å°† RNN éšè—çŠ¶æ€ æ˜ å°„åˆ° è¯æ±‡è¡¨å¤§å°çš„å‘é‡ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚

    def forward(self, x, hidden=None):
        #è¾“å…¥ x å½¢çŠ¶ï¼š(batch_size, seq_len)
        #è¾“å‡º embedded å½¢çŠ¶ï¼š(batch_size, seq_len, embed_dim)
        embedded = self.embedding(x)
        #è¾“å…¥ embedded å½¢çŠ¶ï¼š(batch_size, seq_len, embed_dim)
        #è¾“å‡º output å½¢çŠ¶ï¼š(batch_size, seq_len, hidden_dim)ï¼ˆæ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰
        #è¾“å‡º hidden å½¢çŠ¶ï¼š(num_layers, batch_size, hidden_dim)ï¼ˆæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰
        output, hidden = self.rnn(embedded, hidden) 
        #åªå– æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ output[:, -1, :] ä½œä¸ºè¾“å…¥
        #é€šè¿‡å…¨è¿æ¥å±‚ self.fc å°†éšè—çŠ¶æ€è½¬æ¢ä¸ºè¯æ±‡è¡¨å¤§å°çš„åˆ†å¸ƒï¼ˆç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼‰
        #æœ€ç»ˆ output å½¢çŠ¶ï¼š(batch_size, vocab_size)
        output = self.fc(output[:, -1, :])
        return output, hidden
```

å®šä¹‰æ¨¡å‹æ‰€éœ€å‚æ•°ã€å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

```python
embed_dim = 128
hidden_dim = 512  
vocab_size = len(vocab)

model = RNNTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  
```

### RNN æ¨¡å‹è®­ç»ƒ

RNN è®­ç»ƒè¿‡ç¨‹

```python
def train_model(model, train_loader, epochs=5):
    model.train()# å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    for epoch in range(epochs):
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs}")# ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡
        epoch_grad_norm = None

        for X_batch, Y_batch in progress_bar:
            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)# å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰
            optimizer.zero_grad()# æ¸…ç©ºä¸Šä¸€è½®çš„æ¢¯åº¦ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯

            output, _ = model(X_batch)# å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æ¨¡å‹è¾“å‡º
            loss = criterion(output, Y_batch) # è®¡ç®—æŸå¤±å‡½æ•°å€¼
            loss.backward()# åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦

            optimizer.step() # æ›´æ–°æ¨¡å‹å‚æ•°
            total_loss += loss.item()# ç´¯åŠ å½“å‰ batch çš„æŸå¤±å€¼
            progress_bar.set_postfix(loss=loss.item())# åœ¨è¿›åº¦æ¡ä¸Šæ˜¾ç¤ºå½“å‰ batch çš„æŸå¤±å€¼

        print(f"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}")
        # è®¡ç®—å¹¶è¾“å‡ºæœ¬è½®è®­ç»ƒçš„å¹³å‡æŸå¤±

# è®­ç»ƒæ¨¡å‹
train_model(model, train_loader, epochs=20)  
```

### RNN æ¨¡å‹æµ‹è¯•

RNN ç”Ÿæˆæ–‡æœ¬æµ‹è¯•

```python
def generate_text(model, start_text, num_words=100, temperature=1.0):
    model.eval()# å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨ dropout å’Œ batch normalization
    words = tokenize(start_text)# å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè·å–åˆå§‹è¯åˆ—è¡¨
    input_seq = numericalize(start_text).unsqueeze(0).to(device)
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œå¹¶è°ƒæ•´å½¢çŠ¶ä»¥ç¬¦åˆæ¨¡å‹è¾“å…¥æ ¼å¼ï¼ˆå¢åŠ  batch ç»´åº¦ï¼‰ï¼Œå†ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰

    hidden = None

    for _ in range(num_words): # ç”Ÿæˆ num_words ä¸ªå•è¯
        with torch.no_grad(): # åœ¨æ¨ç†æ—¶å…³é—­æ¢¯åº¦è®¡ç®—ï¼Œæé«˜æ•ˆç‡
            output, hidden = model(input_seq, hidden)# å‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹è¾“å‡ºå’Œæ–°çš„éšè—çŠ¶æ€

        # è®¡ç®— softmaxï¼Œå¹¶åº”ç”¨æ¸©åº¦ç³»æ•°
        logits = output.squeeze(0) / temperature # å¯¹ logits é™¤ä»¥ temperature è°ƒèŠ‚æ¦‚ç‡åˆ†å¸ƒçš„å¹³æ»‘åº¦
        probs = F.softmax(logits, dim=-1) # è®¡ç®— softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ

        # é‡‡æ ·æ–°è¯
        predicted_id = torch.multinomial(probs, num_samples=1).item()
        # åŸºäºæ¦‚ç‡åˆ†å¸ƒ éšæœºé‡‡æ ·ä¸€ä¸ªè¯çš„ç´¢å¼•

        next_word = vocab[predicted_id]  # ä»è¯è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„å•è¯
        words.append(next_word)# å°†ç”Ÿæˆçš„å•è¯æ·»åŠ åˆ°æ–‡æœ¬åˆ—è¡¨ä¸­

        # æ›´æ–°è¾“å…¥åºåˆ—ï¼ˆå°†æ–°è¯åŠ å…¥ï¼Œå¹¶ç§»é™¤æœ€æ—§çš„è¯ï¼Œç»´æŒè¾“å…¥é•¿åº¦ï¼‰
        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],
                              dim=1)

    return " ".join(words) 

# ç”Ÿæˆæ–‡æœ¬
print("\nGenerated Text:")
test_text = dataset["test"][1]["text"]
# å–å‰ 100 ä¸ªå•è¯ä½œä¸ºå‰ç¼€
test_prefix = " ".join(test_text.split()[:100])

# è®©æ¨¡å‹åŸºäºè¯¥å‰ç¼€ç”Ÿæˆ 100 ä¸ªè¯
generated_text = generate_text(model, test_prefix, 100, temperature=0.8)

print("\nğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š\n")
print(generated_text)
```

### å›°æƒ‘åº¦è¯„ä¼°

**1. åŸºæœ¬æ¦‚å¿µ**
å›°æƒ‘åº¦ï¼ˆPerplexity, PPLï¼‰æ˜¯è¡¡é‡è¯­è¨€æ¨¡å‹å¥½åçš„ä¸€ä¸ªå¸¸è§æŒ‡æ ‡ï¼Œå®ƒè¡¨ç¤ºæ¨¡å‹å¯¹æµ‹è¯•æ•°æ®çš„ä¸ç¡®å®šæ€§ï¼Œå³æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶çš„å›°æƒ‘ç¨‹åº¦ã€‚
å¦‚æœä¸€ä¸ªæ¨¡å‹çš„å›°æƒ‘åº¦è¶Šä½ï¼Œè¯´æ˜å®ƒå¯¹æ•°æ®çš„é¢„æµ‹è¶Šå‡†ç¡®ï¼Œå³æ›´â€œç¡®ä¿¡â€è‡ªå·±ç”Ÿæˆçš„è¯è¯­ï¼›å¦‚æœå›°æƒ‘åº¦é«˜ï¼Œè¯´æ˜æ¨¡å‹çš„é¢„æµ‹ä¸å¤ªç¡®å®šï¼Œå¯èƒ½åœ¨å¤šä¸ªè¯ä¹‹é—´æ‘‡æ‘†ä¸å®šã€‚

**2. æ•°å­¦å®šä¹‰**

å‡è®¾ä¸€ä¸ªå¥å­ç”±$N$ä¸ªå•è¯ç»„æˆï¼š

$$W=(w_1,w_2,...,w_N)L_{total}(\mathbf{w}, b) = L_{original}(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2$$

æ¨¡å‹ç»™å‡ºçš„æ¦‚ç‡ä¸ºï¼š

$$P(W)=P(w_1,w_2,...,w_N)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_N|w_1,...,w_{N-1})$$

é‚£ä¹ˆï¼Œå›°æƒ‘åº¦ï¼ˆPerplexity, PPLï¼‰å®šä¹‰ä¸ºï¼š

$$
PPL=P(W)^{-\frac{1}{N}}
$$

æˆ–è€…ç­‰ä»·åœ°ï¼š

$$
PPL = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, ..., w_{i-1}) \right)
$$

å…¶ä¸­ï¼š
- $P(w_i | w_1, ..., w_{i-1})$ æ˜¯æ¨¡å‹åœ¨ç»™å®šå‰ $i-1$ ä¸ªå•è¯æ—¶é¢„æµ‹ $w_i$ çš„æ¦‚ç‡
- $N$ æ˜¯å¥å­çš„å•è¯æ€»æ•°

å›°æƒ‘åº¦çš„æœ€å¥½çš„ç†è§£æ˜¯â€œä¸‹ä¸€ä¸ªè¯å…ƒçš„å®é™…é€‰æ‹©æ•°çš„è°ƒå’Œå¹³å‡æ•°â€ã€‚

- åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯å®Œç¾åœ°ä¼°è®¡æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º1ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º1ã€‚

- åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯é¢„æµ‹æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º0ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦æ˜¯æ­£æ— ç©·å¤§ã€‚

ä¸‹é¢è¯·ä½ æŒ‰ç…§è¦æ±‚è¡¥å…¨è®¡ç®—å›°æƒ‘åº¦çš„ä»£ç 

```python
def compute_perplexity(model, test_text, vocab_dict, seq_len=100):
    """
    è®¡ç®—ç»™å®šæ–‡æœ¬çš„å›°æƒ‘åº¦ï¼ˆPerplexity, PPLï¼‰

    :param model: è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹ï¼ˆRNN/LSTMï¼‰
    :param test_text: éœ€è¦è¯„ä¼°çš„æ–‡æœ¬
    :param vocab_dict: è¯æ±‡è¡¨ï¼ˆç”¨äºè½¬æ¢æ–‡æœ¬åˆ°ç´¢å¼•ï¼‰
    :param seq_len: è¯„ä¼°æ—¶çš„çª—å£å¤§å°
    :return: PPL å›°æƒ‘åº¦
    """
    model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    words = test_text.lower().split()

    # å°†æ–‡æœ¬è½¬æ¢ä¸º token IDï¼Œå¦‚æœè¯ä¸åœ¨è¯è¡¨ä¸­ï¼Œåˆ™ä½¿ç”¨ "<unk>"ï¼ˆæœªçŸ¥è¯ï¼‰å¯¹åº”çš„ç´¢å¼•
    token_ids = torch.tensor([vocab_dict.get(word, vocab_dict["<unk>"]) for word in words], dtype=torch.long)

    # è®¡ç®— PPL
    total_log_prob = 0
    num_tokens = len(token_ids) - 1  # é¢„æµ‹ num_tokens æ¬¡

    with torch.no_grad():
        for i in range(num_tokens):
            """éå†æ–‡æœ¬çš„æ¯ä¸ª tokenï¼Œè®¡ç®—å…¶æ¡ä»¶æ¦‚ç‡ï¼Œæœ€åç´¯åŠ logæ¦‚ç‡"""
            input_seq = token_ids[max(0, i - seq_len):i].unsqueeze(0).to(device)  # è·å–å‰ seq_len ä¸ªå•è¯
            if input_seq.shape[1] == 0:  # é¿å… RNN è¾“å…¥ç©ºåºåˆ—
                continue
            
            target_word = token_ids[i].unsqueeze(0).to(device)  # ç›®æ ‡å•è¯

            # TODO: å‰å‘ä¼ æ’­ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„ logits
            # TODO: è®¡ç®— softmax å¹¶å– log æ¦‚ç‡
            # TODO: å–ç›®æ ‡è¯çš„å¯¹æ•°æ¦‚ç‡
            # TODO: ç´¯åŠ  log æ¦‚ç‡      

    avg_log_prob = total_log_prob / num_tokens  # è®¡ç®—å¹³å‡ log æ¦‚ç‡
    perplexity = torch.exp(torch.tensor(-avg_log_prob)) # è®¡ç®— PPLï¼Œå…¬å¼ PPL = exp(-avg_log_prob)

    return perplexity.item()


# ç¤ºä¾‹ç”¨æ³•
ppl = compute_perplexity(model, generated_text, vocab_dict)
print(f"Perplexity (PPL): {ppl:.4f}")
```



!!! question "æ€è€ƒé¢˜"
    æ€è€ƒé¢˜4ï¼šå‡è®¾ä½ åœ¨RNNå’ŒLSTMè¯­è¨€æ¨¡å‹ä¸Šåˆ†åˆ«è®¡ç®—äº†å›°æƒ‘åº¦ï¼Œå‘ç°RNNçš„PPLæ›´ä½ã€‚è¿™æ˜¯å¦æ„å‘³ç€RNNç”Ÿæˆçš„æ–‡æœ¬ä¸€å®šæ›´æµç•…è‡ªç„¶ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œåœ¨ä»€ä¹ˆæƒ…å†µä¸‹è¿™ä¸¤ä¸ªå›°æƒ‘åº¦å¯ä»¥ç›´æ¥æ¯”è¾ƒï¼Ÿ

    æ€è€ƒé¢˜5ï¼šå›°æƒ‘åº¦æ˜¯ä¸æ˜¯è¶Šä½è¶Šå¥½ï¼Ÿ


## **3. LSTMå’ŒGRUæ–‡æœ¬ç”Ÿæˆå®éªŒ**

!!! abstract "LSTMæ–‡æœ¬ç”Ÿæˆæ¦‚è¿°"
    LSTMï¼ˆLong Short-Term Memoryï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„ RNNï¼Œèƒ½å¤Ÿé€šè¿‡ é—¨æ§æœºåˆ¶ï¼ˆé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨ï¼‰ æœ‰æ•ˆæ•æ‰é•¿æœŸä¾èµ–å…³ç³»ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä½¿å…¶åœ¨å¤„ç†é•¿åºåˆ—ä»»åŠ¡æ—¶æ¯”æ™®é€š RNN æ›´å¼ºå¤§ã€‚
    æœ¬å®éªŒä¾æ—§ä»¥AG News æ•°æ®ä¸ºä¾‹ï¼Œç»™å®šå‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚


![ç¤ºä¾‹å›¾ç‰‡](pics/lstm.png)

æ–‡æœ¬çš„é¢„å¤„ç† è®­ç»ƒæ•°æ®ç”Ÿæˆä¸å‰é¢ä¸€è‡´


### LSTM æ¨¡å‹æ„å»º

å®ç°äº†ä¸€ä¸ªåŸºäº LSTM çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚

```python
class LSTMTextGenerator(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):
        super(LSTMTextGenerator, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden=None):
        embedded = self.embedding(x)  # (B, L, embed_dim)
        output, hidden = self.lstm(embedded, hidden)  # (B, L, hidden_dim)
        output = self.fc(output[:, -1, :])  # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œé¢„æµ‹
        return output, hidden
```

å®šä¹‰æ¨¡å‹æ‰€éœ€å‚æ•°ã€å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

```python
embed_dim = 128
hidden_dim = 512  
vocab_size = len(vocab)

model = LSTMTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  
```

### LSTM æ¨¡å‹è®­ç»ƒ

LSTM è®­ç»ƒè¿‡ç¨‹

```python
def train_model(model, train_loader, epochs=5):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs}")
        epoch_grad_norm = None

        for X_batch, Y_batch in progress_bar:
            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)
            optimizer.zero_grad()

            output, _ = model(X_batch)
            loss = criterion(output, Y_batch)
            loss.backward()

            optimizer.step()
            total_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

        print(f"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}")

# è®­ç»ƒæ¨¡å‹
train_model(model, train_loader, epochs=20)
```

### LSTM æ¨¡å‹æµ‹è¯•

LSTM ç”Ÿæˆæ–‡æœ¬æµ‹è¯•

```python
def generate_text(model, start_text, num_words=100, temperature=1.0):
    model.eval()
    words = tokenize(start_text)
    input_seq = numericalize(start_text).unsqueeze(0).to(device)
    hidden = None

    for _ in range(num_words):
        with torch.no_grad():
            output, hidden = model(input_seq, hidden)

        # è®¡ç®— softmaxï¼Œå¹¶åº”ç”¨æ¸©åº¦ç³»æ•°
        logits = output.squeeze(0) / temperature
        probs = F.softmax(logits, dim=-1)

        # é‡‡æ ·æ–°è¯
        predicted_id = torch.multinomial(probs, num_samples=1).item()

        next_word = vocab[predicted_id]
        words.append(next_word)

        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],
                              dim=1)

    return " ".join(words) 

# ç”Ÿæˆæ–‡æœ¬
print("\nGenerated Text:")
test_text = dataset["test"][1]["text"]
# å–å‰ 100 ä¸ªå•è¯ä½œä¸ºå‰ç¼€
test_prefix = " ".join(test_text.split()[:100])

# è®©æ¨¡å‹åŸºäºè¯¥å‰ç¼€ç”Ÿæˆ 100 ä¸ªè¯
generated_text = generate_text(model, test_prefix, 100, temperature=0.8)
print("\nğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š\n")
print(generated_text)
```

å€ŸåŠ©RNNæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è®¡ç®—å›°æƒ‘åº¦çš„å‡½æ•°ï¼Œè®¡ç®—ä¸€ä¸‹lstmåœ¨generated_textä¸Šçš„å›°æƒ‘åº¦ã€‚

!!! question "æ€è€ƒé¢˜"
    æ€è€ƒé¢˜6ï¼šè§‚å¯Ÿä¸€ä¸‹RNNå’ŒLSTMè®­ç»ƒè¿‡ç¨‹ä¸­lossçš„å˜åŒ–ï¼Œå¹¶åˆ†æä¸€ä¸‹é€ æˆè¿™ç§ç°è±¡çš„åŸå› ã€‚



!!! abstract "GRUæ–‡æœ¬ç”Ÿæˆæ¦‚è¿°"
    GRUï¼ˆGated Recurrent Unitï¼‰æ˜¯ LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ æ›´æ–°é—¨ï¼ˆUpdate Gateï¼‰å’Œé‡ç½®é—¨ï¼ˆReset Gateï¼‰ æ¥æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œä¸”èƒ½åœ¨è®¸å¤šä»»åŠ¡ä¸­å–å¾—ä¸ LSTM ç›¸ä¼¼çš„æ•ˆæœï¼ŒåŒæ—¶å‡å°‘è®¡ç®—æˆæœ¬å’Œå‚æ•°é‡ã€‚
    æœ¬å®éªŒä¾æ—§ä»¥AG News æ•°æ®ä¸ºä¾‹ï¼Œç»™å®šå‰100ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚


![ç¤ºä¾‹å›¾ç‰‡](pics/GRU.png)


æ–‡æœ¬çš„é¢„å¤„ç† è®­ç»ƒæ•°æ®ç”Ÿæˆä¸å‰é¢ä¸€è‡´


### GRU æ¨¡å‹æ„å»º

å®ç°äº†ä¸€ä¸ªåŸºäº GRU çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚

```python
class GRUTextGenerator(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):
        super(GRUTextGenerator, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden=None):
        embedded = self.embedding(x)  # (B, L, embed_dim)
        output, hidden = self.gru(embedded, hidden)  # (B, L, hidden_dim)
        output = self.fc(output[:, -1, :])  # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œé¢„æµ‹
        return output, hidden
```

å®šä¹‰æ¨¡å‹æ‰€éœ€å‚æ•°ã€å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

```python
embed_dim = 128
hidden_dim = 512  
vocab_size = len(vocab)

model = GRUTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

### GRU æ¨¡å‹è®­ç»ƒ

GRU è®­ç»ƒè¿‡ç¨‹ä¹Ÿä¸LSTMä¿æŒä¸€è‡´


### GRU æ¨¡å‹æµ‹è¯•

GRU ç”Ÿæˆæ–‡æœ¬æµ‹è¯•

```python
def generate_text(model, start_text, num_words=100, temperature=1.0):
    model.eval()
    words = tokenize(start_text)
    input_seq = numericalize(start_text).unsqueeze(0).to(device)
    hidden = None

    for _ in range(num_words):
        with torch.no_grad():
            output, hidden = model(input_seq, hidden)

        # è®¡ç®— softmaxï¼Œå¹¶åº”ç”¨æ¸©åº¦ç³»æ•°
        logits = output.squeeze(0) / temperature
        probs = F.softmax(logits, dim=-1)

        # é‡‡æ ·æ–°è¯
        predicted_id = torch.multinomial(probs, num_samples=1).item()

        next_word = vocab[predicted_id]
        words.append(next_word)

        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],
                              dim=1)

    return " ".join(words) 

# ç”Ÿæˆæ–‡æœ¬
print("\nGenerated Text:")
test_text = dataset["test"][1]["text"]
# å–å‰ 100 ä¸ªå•è¯ä½œä¸ºå‰ç¼€
test_prefix = " ".join(test_text.split()[:100])

# è®©æ¨¡å‹åŸºäºè¯¥å‰ç¼€ç”Ÿæˆ 100 ä¸ªè¯
generated_text = generate_text(model, test_prefix, 100, temperature=0.8)
print("\nğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š\n")
print(generated_text)
```

å€ŸåŠ©RNNæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è®¡ç®—å›°æƒ‘çš„å‡½æ•°ï¼Œè®¡ç®—ä¸€ä¸‹GRUåœ¨generated_textä¸Šçš„å›°æƒ‘åº¦ã€‚

!!! question "æ€è€ƒé¢˜"
    æ€è€ƒé¢˜7ï¼šè¿™ä¸‰ä¸ªå›°æƒ‘åº¦å¯ä»¥ç›´æ¥æ¯”è¾ƒå—ï¼Ÿåˆ†æä¸€ä¸‹ã€‚  

    æ€è€ƒé¢˜8ï¼šGRU åªæœ‰ä¸¤ä¸ªé—¨ï¼ˆæ›´æ–°é—¨å’Œé‡ç½®é—¨ï¼‰ï¼Œç›¸æ¯” LSTM å°‘äº†ä¸€ä¸ªé—¨æ§å•å…ƒï¼Œè¿™æ ·çš„è®¾è®¡æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ 

    æ€è€ƒé¢˜9ï¼šåœ¨ä½ç®—åŠ›è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºï¼‰ä¸Šï¼ŒRNNã€LSTM å’Œ GRU å“ªä¸ªæ›´é€‚åˆéƒ¨ç½²ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

    æ€è€ƒé¢˜10ï¼šå¦‚æœå°±æ˜¯è¦ä½¿ç”¨RNNæ¨¡å‹ï¼ŒåŸå…ˆçš„ä»£ç è¿˜æœ‰å“ªé‡Œå¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹ï¼Ÿè¯·ç»™å‡ºä¿®æ”¹éƒ¨åˆ†ä»£ç ä»¥åŠå®éªŒç»“æœã€‚


