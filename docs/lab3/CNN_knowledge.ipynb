{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3798751b",
   "metadata": {},
   "source": [
    "# 图像卷积\n",
    "## 互相关运算\n",
    "在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。 \n",
    "当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，\n",
    "得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。 \n",
    "\n",
    "接下来，我们在corr2d函数中实现如上过程，该函数接受输入张量X和卷积核张量K，\n",
    "并返回输出张量Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c4fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534d0ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16b39f",
   "metadata": {},
   "source": [
    "# 填充和步幅\n",
    "本节我们将介绍填充（padding）和步幅（stride）。假设以下情景： \n",
    "有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。\n",
    "这是由于卷积核的宽度和高度通常大于\n",
    "所导致的。比如，一个240x240 像素的图像，经过10层 5x5的卷积后，将减少到200x200\n",
    "像素。如此一来，原始图像的边界丢失了许多有用信息。\n",
    "而填充是解决此问题最有效的方法； \n",
    "有时，我们可能希望大幅降低图像的宽度和高度。例如，\n",
    "如果我们发现原始的输入分辨率十分冗余。步幅则可以在这类情况下提供帮助。\n",
    "\n",
    "## 填充\n",
    "如上所述，在应用多层卷积时，我们常常丢失边缘像素。 由于我们通常使用小卷积核，\n",
    "因此对于任何单个卷积，我们可能只会丢失几个像素。 但随着我们应用许多连续卷积层，\n",
    "累积丢失的像素数就多了。 解决这个问题的简单方法即为填充（padding）：\n",
    "在输入图像的边界填充元素（通常填充元素是 0）。 \n",
    "假设我们原张量形状为nxn，卷积核为kxk，我们在原张量的基础上填充A行，在原张量基础上\n",
    "填充B列，那么在进行互相关运算后，得到的输出形状为(n+A-k+1)x(n+B-K+1),\n",
    "在图中的例子中，输出初始是3x3，我们在填充了两行（上下各一行）以及两列，那么新的张量\n",
    "为5x5， 通过运算后，得到形状为(5-2+1)x(5-2+1)\n",
    "\n",
    "在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素。\n",
    "给定高度和宽度为8的输入，则输出的高度和宽度也是8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad9cf291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    # 这里的（1，1）表示批量大小和通道数都是1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # 省略前两个维度：批量大小和通道\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893ba1c",
   "metadata": {},
   "source": [
    "## 步幅\n",
    "在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中\n",
    "，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，\n",
    "卷积窗口可以跳过中间位置，每次滑动多个元素。\n",
    "\n",
    "我们将每次滑动元素的数量称为步幅（stride）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22fc747e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9120027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c667276",
   "metadata": {},
   "source": [
    "# 多输入输出通道\n",
    "彩色图像具有标准的RGB通道来代表红、绿和蓝。 但是到目前为止，我们仅展示了单个输入\n",
    "和单个输出通道的简化例子。 这使得我们可以将输入、卷积核和输出看作二维张量。\n",
    "\n",
    "当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像\n",
    "具有3 x h x w 的形状。我们将这个大小为3的轴称为通道（channel）维度。\n",
    "本节将更深入地研究具有多输入和多输出通道的卷积核。\n",
    "## 多输入通道\n",
    "\n",
    "当输入包含多个通道时，需要构造一个与输入数据具有\n",
    "相同输入通道数的卷积核，以便与输入数据进行互相关运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04cc4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "def corr2d_multi_in(X, K):\n",
    "    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起\n",
    "    return sum(corr2d(x, k) for x, k in zip(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5942e471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4878e",
   "metadata": {},
   "source": [
    "## 多输出通道\n",
    "到目前为止，不论有多少输入通道，我们还只有一个输出通道。在最流行的神经网络架构中\n",
    "，随着神经网络层数的加深， 我们常会增加输出通道的维数，\n",
    "通过减少空间分辨率以获得更大的通道深度。\n",
    "如下所示，我们实现一个计算多个通道的输出的互相关函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eabfbfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "    # 最后将所有结果都叠加在一起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n",
    "\n",
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfc7b1",
   "metadata": {},
   "source": [
    "下面，我们对输入张量X与卷积核张量K执行互相关运算。现在的输出包含3\n",
    "个通道，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "535b0d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81e24d",
   "metadata": {},
   "source": [
    "# 汇聚层\n",
    "\n",
    "通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，\n",
    "这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。\n",
    "\n",
    "本节将介绍汇聚（pooling）层，它具有双重目的：\n",
    "降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。\n",
    "\n",
    "## 最大汇聚层和平均汇聚层\n",
    "与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的\n",
    "所有区域上滑动，为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出。 \n",
    "然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。\n",
    "相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。\n",
    "这些操作分别称为最大汇聚层（maximum pooling）和平均汇聚层（average pooling）。\n",
    "\n",
    "在这两种情况下，与互相关运算符一样，\n",
    "汇聚窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。\n",
    "在汇聚窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。\n",
    "计算最大值或平均值是取决于使用了最大汇聚层还是平均汇聚层。\n",
    "\n",
    "在下面的代码中的pool2d函数，我们实现汇聚层的前向传播。\n",
    "然而，这里我们没有卷积核，输出为输入中每个区域的最大值或平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae3ea59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9019862",
   "metadata": {},
   "source": [
    "构造输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d98d6c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0a349",
   "metadata": {},
   "source": [
    "此外，我们还可以验证平均汇聚层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91342990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e988742",
   "metadata": {},
   "source": [
    "## 填充与步幅\n",
    "\n",
    "与卷积层一样，汇聚层也可以改变输出形状。和以前一样，\n",
    "我们可以通过填充和步幅以获得所需的输出形状。 \n",
    "下面，我们用深度学习框架中内置的二维最大汇聚层，\n",
    "来演示汇聚层中填充和步幅的使用。 \n",
    "我们首先构造了一个输入张量X，它有四个维度，其中样本数和通道数都是1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38475bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3486a",
   "metadata": {},
   "source": [
    "默认情况下，深度学习框架中的步幅与汇聚窗口的大小相同。 \n",
    "因此，如果我们使用形状为(3, 3)的汇聚窗口，\n",
    "那么默认情况下，我们得到的步幅形状为(3, 3)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c30625b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd331634",
   "metadata": {},
   "source": [
    "## 多个通道\n",
    "在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，\n",
    "而不是像卷积层一样在通道上对输入进行汇总。\n",
    "这意味着汇聚层的输出通道数与输入通道数相同。 \n",
    "下面，我们将在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39fe7e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.cat((X, X + 1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e234bb",
   "metadata": {},
   "source": [
    "如下所示，汇聚后输出通道的数量仍然是2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40dbeb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444588c",
   "metadata": {},
   "source": [
    "## 练习  图像中目标的边缘检测\n",
    "如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。 首先，我们构造一个\n",
    "6x8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8eb467a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561bdebd",
   "metadata": {},
   "source": [
    "接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2e85a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b907cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y\n",
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31832e81",
   "metadata": {},
   "source": [
    "现在我们将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核K只可以检测垂直边缘，无法检测水平边缘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "512e851e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd64cf",
   "metadata": {},
   "source": [
    "1.请使用Conv2D类,补全训练部分代码，使得训练得到的卷积核参数接近torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a9e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6e24fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 3.729\n",
      "epoch 4, loss 0.695\n",
      "epoch 6, loss 0.145\n",
      "epoch 8, loss 0.036\n",
      "epoch 10, loss 0.011\n"
     ]
    }
   ],
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），\n",
    "# 其中批量大小和通道数都为1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "#训练补全部分(自定义损失函数，训练epoch，学习率等)\n",
    "#TODU\n",
    "#TODU\n",
    "#TODU\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b367933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9997, -0.9810]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))\n",
    "#要求此输出接近torch.tensor([[1.0, -1.0]])，\n",
    "# 如tensor([[ 0.9997, -0.9810]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb693e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
