{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化和激活函数探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数实验\n",
    "\n",
    "激活函数是神经网络中的重要组成部分，它为网络引入非线性变换能力。本实验将探索三种常见的激活函数(ReLU、Sigmoid、Tanh)的特性及其对模型训练的影响。\n",
    "\n",
    "### 激活函数的实现与可视化\n",
    "\n",
    "首先我们实现这三个激活函数及其导数，并通过可视化来理解它们的特性：\n",
    "\n",
    "- ReLU: f(x) = max(0,x)\n",
    "- Sigmoid: f(x) = 1/(1+e^(-x))  \n",
    "- Tanh: f(x) = (e^x - e^(-x))/(e^x + e^(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_activation_function(MyReLU,MySigmoid,MyTanh):\n",
    "    x = torch.linspace(-10, 10, 1000)\n",
    "\n",
    "    # 使用自己实现的激活函数\n",
    "    my_relu = MyReLU()\n",
    "    my_sigmoid = MySigmoid()\n",
    "    my_tanh = MyTanh()\n",
    "\n",
    "    # 计算激活函数值和导数值\n",
    "    y_my_relu = my_relu(x)\n",
    "    y_my_sigmoid = my_sigmoid(x)\n",
    "    y_my_tanh = my_tanh(x)\n",
    "\n",
    "    y_my_relu_derivative = my_relu.derivative(x)\n",
    "    y_my_sigmoid_derivative = my_sigmoid.derivative(x)\n",
    "    y_my_tanh_derivative = my_tanh.derivative(x)\n",
    "    # 使用PyTorch的激活函数作为参考\n",
    "    torch_relu = nn.ReLU()\n",
    "    torch_sigmoid = nn.Sigmoid()\n",
    "    torch_tanh = nn.Tanh()\n",
    "    # 验证实现的正确性\n",
    "    y_torch_relu = torch_relu(x)\n",
    "    y_torch_sigmoid = torch_sigmoid(x)\n",
    "    y_torch_tanh = torch_tanh(x)\n",
    "\n",
    "    # 计算PyTorch激活函数的导数\n",
    "    x_torch = x.clone().requires_grad_(True)  # 使x支持求导\n",
    "\n",
    "    # ReLU导数\n",
    "    y_torch_relu = torch_relu(x_torch)\n",
    "    y_torch_relu.sum().backward()\n",
    "    y_torch_relu_derivative = x_torch.grad.clone()\n",
    "    x_torch.grad = None  # 清除梯度\n",
    "\n",
    "    # Sigmoid导数\n",
    "    y_torch_sigmoid = torch_sigmoid(x_torch)\n",
    "    y_torch_sigmoid.sum().backward()\n",
    "    y_torch_sigmoid_derivative = x_torch.grad.clone()\n",
    "    x_torch.grad = None\n",
    "\n",
    "    # Tanh导数\n",
    "    y_torch_tanh = torch_tanh(x_torch)\n",
    "    y_torch_tanh.sum().backward()\n",
    "    y_torch_tanh_derivative = x_torch.grad.clone()\n",
    "\n",
    "    assert torch.allclose(y_my_relu, y_torch_relu, rtol=1e-4, atol=1e-4), \"ReLU实现有误\"\n",
    "    assert torch.allclose(y_my_sigmoid, y_torch_sigmoid, rtol=1e-4, atol=1e-4), \"Sigmoid实现有误\"\n",
    "    assert torch.allclose(y_my_tanh, y_torch_tanh, rtol=1e-4, atol=1e-4), \"Tanh实现有误\"\n",
    "\n",
    "    assert torch.allclose(y_my_relu_derivative, y_torch_relu_derivative, rtol=1e-4, atol=1e-4), \"ReLU导数实现有误\"\n",
    "    assert torch.allclose(y_my_sigmoid_derivative, y_torch_sigmoid_derivative, rtol=1e-4, atol=1e-4), \"Sigmoid导数实现有误\"\n",
    "    assert torch.allclose(y_my_tanh_derivative, y_torch_tanh_derivative, rtol=1e-4, atol=1e-4), \"Tanh导数实现有误\"\n",
    "\n",
    "    # 绘制激活函数和导数\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # 绘制激活函数\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x.numpy(), y_my_relu.numpy(), label='ReLU')\n",
    "    plt.plot(x.numpy(), y_my_sigmoid.numpy(), label='Sigmoid')\n",
    "    plt.plot(x.numpy(), y_my_tanh.numpy(), label='Tanh')\n",
    "    plt.title('Activation Functions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 绘制导数\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x.numpy(), y_my_relu_derivative.numpy(), label='ReLU\\'')\n",
    "    plt.plot(x.numpy(), y_my_sigmoid_derivative.numpy(), label='Sigmoid\\'')\n",
    "    plt.plot(x.numpy(), y_my_tanh_derivative.numpy(), label='Tanh\\'')\n",
    "    plt.title('Derivatives of Activation Functions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y\\'')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己实现激活函数及其导数\n",
    "class MyReLU:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"实现ReLU激活函数: f(x) = max(0, x)\"\"\"\n",
    "        # TODO: 实现ReLU函数，返回x中所有元素与0的较大值\n",
    "        # TODO: 可以使用torch.maximum函数实现ReLU\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        \"\"\"ReLU的导数: f'(x) = 1 if x > 0 else 0\"\"\"\n",
    "        # TODO: 实现ReLU的导数，当x>0时为1，否则为0\n",
    "        # TODO: 可以使用torch.where函数实现ReLU的导数\n",
    "\n",
    "class MySigmoid:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"实现Sigmoid激活函数: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "        # TODO: 实现Sigmoid函数，返回x中所有元素的Sigmoid值\n",
    "        # TODO: 可以使用torch.exp函数实现e^(-x)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        \"\"\"Sigmoid的导数: f'(x) = f(x) * (1 - f(x))\"\"\"\n",
    "       # TODO: 实现Sigmoid的导数，提示：可以利用__call__方法\n",
    "\n",
    "class MyTanh:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"实现Tanh激活函数: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\"\"\"\n",
    "        # TODO: 实现Tanh函数，返回x中所有元素的Tanh值\n",
    "        # TODO: 可以使用torch.exp函数实现e^(-x)\n",
    "        \n",
    "    \n",
    "    def derivative(self, x):\n",
    "        \"\"\"Tanh的导数: f'(x) = 1 - f(x)^2\"\"\"\n",
    "        # TODO: 实现Tanh的导数，提示：可以利用__call__方法\n",
    "\n",
    "# 测试实现的激活函数\n",
    "\n",
    "check_activation_function(MyReLU,MySigmoid,MyTanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题1：为什么神经网络需要非线性激活函数？如果使用线性激活函数会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 梯度消失问题实验\n",
    "\n",
    "梯度消失是深度神经网络训练中的一个常见问题。当网络层数较深时，在反向传播过程中梯度会随着层数的增加而逐渐减小，导致靠近输入层的参数几乎无法更新。本实验将：\n",
    "\n",
    "1. 构建一个多层神经网络\n",
    "2. 分别使用不同激活函数(ReLU、Sigmoid、Tanh)\n",
    "3. 观察训练过程中各层的梯度分布\n",
    "4. 分析不同激活函数对梯度消失问题的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 梯度消失实验的网络\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation_name, num_layers=5):\n",
    "        super(Network, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # 创建很深的网络来测试梯度问题\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.Linear(100, 100))\n",
    "\n",
    "        if activation_name == 'relu': \n",
    "            self.activation = MyReLU()\n",
    "        elif activation_name == 'sigmoid':\n",
    "            self.activation = MySigmoid()\n",
    "        elif activation_name == 'tanh':\n",
    "            self.activation = MyTanh()\n",
    "        self.input_layer = nn.Linear(input_dim, 100)\n",
    "        self.output_layer = nn.Linear(100, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: 实现前向传播函数\n",
    "        # 1. 首先通过输入层\n",
    "        # 2. 应用激活函数\n",
    "        # 3. 依次通过每个隐藏层并应用激活函数\n",
    "        # 4. 最后通过输出层返回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(model):\n",
    "    \"\"\"分析模型各层的梯度分布\"\"\"\n",
    "    gradients_by_layer = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            # 计算每层梯度的统计信息\n",
    "            layer_grads = param.grad.cpu().numpy()\n",
    "            grad_mean = np.mean(np.abs(layer_grads))\n",
    "            grad_std = np.std(layer_grads)\n",
    "            gradients_by_layer.append({\n",
    "                'layer': name,\n",
    "                'mean': grad_mean,\n",
    "                'std': grad_std\n",
    "            })\n",
    "    return gradients_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size=64):\n",
    "    # 准备数据\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader,test_loader\n",
    "    \n",
    "def plot_results(results):\n",
    "    # 绘制结果\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 绘制准确率随epoch变化\n",
    "    for activation in results:\n",
    "        ax1.plot(results[activation]['acc_history'], label=f'{activation}')\n",
    "    ax1.set_title('Training Accuracy over Epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 绘制最后一个epoch的梯度分布\n",
    "    for activation in results:\n",
    "        last_grads = results[activation]['grad_history'][-1]\n",
    "        ax2.semilogy(last_grads, label=f'{activation}')\n",
    "    ax2.set_title('Gradient Distribution (Last Epoch)')\n",
    "    ax2.set_xlabel('Layer Index')\n",
    "    ax2.set_ylabel('Gradient Mean (log scale)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment_gradient_vanishing():\n",
    "    \"\"\"梯度消失实验\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 训练不同激活函数的深层网络\n",
    "    results = {activation: {'acc_history': [], 'grad_history': []} for activation in ['relu', 'sigmoid', 'tanh']}\n",
    "    \n",
    "    n_epochs = 5\n",
    "    for activation in ['relu','sigmoid', 'tanh']:\n",
    "        train_loader,_ = get_data()\n",
    "        model = Network(input_dim=784, output_dim=10, activation_name=activation,num_layers=2).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # 训练阶段\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                data = data.view(data.size(0), -1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                # 收集梯度信息\n",
    "                if batch_idx % 1 == 0:\n",
    "                    gradients = analyze_gradients(model)\n",
    "                    results[activation]['grad_history'].append(\n",
    "                        [g['mean'] for g in gradients]\n",
    "                    )\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # 计算准确率\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                total += target.size(0)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # 计算epoch的平均准确率\n",
    "            epoch_acc = 100. * correct / total\n",
    "            results[activation]['acc_history'].append(epoch_acc)\n",
    "            print(f'Activation: {activation}, Epoch: {epoch}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "    plot_results(results)\n",
    "\n",
    "experiment_gradient_vanishing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题2：观察实验结果，为什么训练准确率会和激活函数选择相关？这与梯度分布有什么关系？\n",
    "\n",
    "提示：\n",
    "- 比较不同激活函数的梯度范围\n",
    "- 分析梯度消失对模型训练的影响\n",
    "- 思考为什么ReLU在深度学习中更受欢迎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU死亡现象实验\n",
    "\n",
    "ReLU死亡现象指神经元在训练过程中持续输出0，导致参数无法更新的问题。本实验将：\n",
    "\n",
    "1. 统计使用ReLU和Tanh激活函数时的神经元激活情况\n",
    "2. 分析ReLU死亡现象的产生原因\n",
    "3. 探讨如何缓解这一问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_relu_death():\n",
    "    \"\"\"ReLU死亡现象实验\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 比较ReLU和LeakyReLU\n",
    "    activation_counts = {'relu': [], 'tanh': []}\n",
    "    \n",
    "    for activation in ['relu', 'tanh']:\n",
    "        model = Network(input_dim=784, output_dim=10, activation_name=activation, num_layers=2).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # 训练几个epoch并记录激活值为0的神经元数量\n",
    "        for epoch in range(5):\n",
    "            zero_activations = 0\n",
    "            total_activations = 0\n",
    "            train_loader,_ = get_data()\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                data = data.view(data.size(0), -1)\n",
    "                optimizer.zero_grad()\n",
    "                # 统计死亡神经元\n",
    "                x = model.input_layer(data)\n",
    "                x = model.activation(x)\n",
    "                \n",
    "                # 记录中间层的激活值\n",
    "                for layer in model.layers:\n",
    "                    x = layer(x)\n",
    "                    activated = model.activation(x)\n",
    "                    \n",
    "                    # 统计激活值为0的神经元数量\n",
    "                    if activation in ['relu', 'tanh']:\n",
    "                        zero_activations += torch.sum(activated == 0).item()\n",
    "                        total_activations += activated.numel()\n",
    "                \n",
    "                # 继续前向传播完成训练\n",
    "                output = model.output_layer(x)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # 计算死亡率\n",
    "            death_rate = zero_activations / total_activations\n",
    "            print(f'Activation: {activation}, Epoch {epoch}: Death Rate: {death_rate:.2f}%')\n",
    "            activation_counts[activation].append(death_rate)\n",
    "    \n",
    "    # 绘制死亡率随时间变化\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for activation, rates in activation_counts.items():\n",
    "        plt.plot(rates, label=activation)\n",
    "    plt.title('ReLU Death Rate During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Death Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "experiment_relu_death()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题3：ReLU死亡现象的成因是什么？有哪些解决方案？\n",
    "\n",
    "提示：\n",
    "- 分析什么情况下神经元会停止更新\n",
    "- 思考学习率、初始化方式的影响\n",
    "- 了解LeakyReLU等变体的优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 正则化方法实验\n",
    "\n",
    "过拟合是机器学习中的常见问题，正则化是缓解过拟合的重要手段。本实验将探索两种主要的正则化方法：L2正则化和Dropout。\n",
    "\n",
    "### L2正则化实验\n",
    "\n",
    "L2正则化通过在损失函数中添加权重的平方项来限制模型复杂度。本实验将：\n",
    "\n",
    "1. 构造一个容易过拟合的数据集\n",
    "2. 实现带有L2正则化的模型训练\n",
    "3. 对比有无正则化的训练效果\n",
    "4. 分析L2正则化对模型参数的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2正则化\n",
    "\n",
    "**1. 基本概念**\n",
    "L2正则化(也称为权重衰减)是深度学习中最常用的正则化技术之一。其核心思想是通过限制模型参数的大小来降低模型复杂度，从而防止过拟合。\n",
    "\n",
    "**2. 数学表达**\n",
    "\n",
    "在原始损失函数基础上添加L2正则项：\n",
    "\n",
    "$$L_{total}(\\mathbf{w}, b) = L_{original}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "其中：\n",
    "- $L_{original}$ 是原始损失函数(如均方误差)\n",
    "- $\\|\\mathbf{w}\\|^2$ 是权重向量的L2范数平方\n",
    "- $\\lambda$ 是正则化系数，控制正则化的强度\n",
    "\n",
    "**3. 工作原理**\n",
    "\n",
    "1. **参数惩罚**：\n",
    "   - L2正则化通过惩罚较大的权重参数，鼓励模型学习更小的权重值\n",
    "   - 较大的权重往往意味着模型对输入特征的依赖程度更高，更容易过拟合\n",
    "\n",
    "2. **平滑效果**：\n",
    "   - L2正则化倾向于将权重均匀分散到所有特征上\n",
    "   - 这使得模型不会过分依赖某些特定特征，提高了泛化能力\n",
    "\n",
    "3. **梯度更新**：\n",
    "   - 在参数更新时，L2正则化项的梯度为 $\\lambda\\mathbf{w}$\n",
    "   - 这相当于在每次更新时将权重缩小一个比例，故称为权重衰减\n",
    "\n",
    "**4. 与L1正则化的对比**\n",
    "\n",
    "- L2正则化倾向于产生值较小但非零的权重，权重呈现正态分布\n",
    "- L1正则化倾向于产生稀疏的权重向量，即许多权重为零\n",
    "- L2正则化在特征之间有关联时更适用，而L1正则化更适合特征选择\n",
    "\n",
    "**5. 实践应用**\n",
    "\n",
    "- 正则化系数 $\\lambda$ 是一个需要调整的超参数\n",
    "- $\\lambda = 0$ 时相当于无正则化\n",
    "- $\\lambda$ 越大，正则化效果越强，模型越简单，但可能欠拟合\n",
    "- 通常通过交叉验证来选择合适的 $\\lambda$ 值\n",
    "\n",
    "#### 数据集定义\n",
    "给定$x$，我们将**使用以下三阶多项式来生成训练和测试数据的标签：**\n",
    "\n",
    "**$$y = 0.05 + \\sum_{i = 1}^d 0.01 x_i + \\epsilon \\text{ where }\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.01^2).$$**\n",
    "\n",
    "我们选择标签是关于输入的线性函数。\n",
    "标签同时被均值为0，标准差为0.01高斯噪声破坏。\n",
    "为了使过拟合的效果更加明显，我们可以将问题的维数增加到$d = 200$，\n",
    "并使用一个只包含20个样本的小训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度和样本数\n",
    "d = 200  # 特征维度\n",
    "n_train, n_test = 20, 100  # 训练和测试数据集大小\n",
    "\n",
    "# 生成特征数据\n",
    "features = np.random.normal(size=(n_train + n_test, d))\n",
    "\n",
    "# 生成标签\n",
    "# y = 0.05 + 0.01 * sum(x_i) + epsilon\n",
    "labels = 0.05 + 0.01 * np.sum(features, axis=1)\n",
    "# 添加噪声 epsilon ~ N(0, 0.01^2)\n",
    "labels += np.random.normal(0, 0.01, size=labels.shape)\n",
    "\n",
    "# 转换为tensor\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# 分割训练集和测试集\n",
    "train_features = features[:n_train]\n",
    "test_features = features[n_train:]\n",
    "train_labels = labels[:n_train]\n",
    "test_labels = labels[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **定义$L_2$范数惩罚**\n",
    "\n",
    "实现这一惩罚最方便的方法是对所有项求平方后并将它们求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    # TODO: 使用torch.sum和.pow方法实现L2范数惩罚\n",
    "    # 提示: 对权重参数w平方求和，再除以2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对模型进行训练和测试\n",
    "\n",
    "让我们**实现一个函数来评估模型在给定数据集上的损失**，及**训练函数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def evaluate_loss(net, data_iter, loss):\n",
    "    \"\"\"评估给定数据集上模型的损失\"\"\"\n",
    "    metric = Accumulator(2)  # 损失的总和,样本数量\n",
    "    for X, y in data_iter:\n",
    "        out = net(X)\n",
    "        y = y.reshape(out.shape)\n",
    "        l = loss(out, y)\n",
    "        metric.add(l.sum(), l.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train_epoch(net, train_iter, loss, trainer, penalty_lambda):\n",
    "    \"\"\"训练模型一个epoch\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    net.train()\n",
    "    # 训练损失总和、训练样本数、实例数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        trainer.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        weight = net[0].weight\n",
    "        l = loss(y_hat, y.reshape(y_hat.shape)) + penalty_lambda * l2_penalty(weight)\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        metric.add(float(l.sum()), y.numel(), 1)\n",
    "    # 返回训练损失和训练准确率\n",
    "    return metric[0] / metric[2]\n",
    "\n",
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=100, penalty_lambda=0):\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    input_shape = train_features.shape[-1]\n",
    "    # 不设置偏置，因为我们已经在多项式中实现了它\n",
    "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    \n",
    "    # 创建数据迭代器\n",
    "    train_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(train_features, train_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    test_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(test_features, test_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.003)\n",
    "    \n",
    "    # 记录训练过程\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(net, train_iter, loss, trainer, penalty_lambda)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            train_l = evaluate_loss(net, train_iter, loss)\n",
    "            test_l = evaluate_loss(net, test_iter, loss)\n",
    "            train_loss.append(train_l)\n",
    "            test_loss.append(test_l)\n",
    "            print(f'epoch {epoch+1}, train loss {train_l:.3f}, test loss {test_l:.3f}')\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(range(1, len(train_loss) * 20 + 1, 20), train_loss, label='train')\n",
    "    plt.semilogy(range(1, len(test_loss) * 20 + 1, 20), test_loss, label='test')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print('weight:', net[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不使用正则化\n",
    "train(train_features, test_features,\n",
    "      train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用L2正则化\n",
    "train(train_features, test_features,\n",
    "      train_labels, test_labels, penalty_lambda=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简洁实现\n",
    "\n",
    "在深度学习框架中，我们无需实现L2正则化，只需要在损失函数中添加正则项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(net, train_iter, loss, trainer):\n",
    "    \"\"\"训练模型一个epoch\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    net.train()\n",
    "    # 训练损失总和、训练样本数、实例数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        trainer.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        weight = net[0].weight\n",
    "        l = loss(y_hat, y.reshape(y_hat.shape))\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        metric.add(float(l.sum()), y.numel(), 1)\n",
    "    # 返回训练损失和训练准确率\n",
    "    return metric[0] / metric[2]\n",
    "\n",
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=100, penalty_lambda=0):\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    input_shape = train_features.shape[-1]\n",
    "    # 不设置偏置，因为我们已经在多项式中实现了它\n",
    "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    \n",
    "    # 创建数据迭代器\n",
    "    train_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(train_features, train_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    test_iter = DataLoader(\n",
    "        torch.utils.data.TensorDataset(test_features, test_labels.reshape(-1,1)),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.003, weight_decay=penalty_lambda)\n",
    "    \n",
    "    # 记录训练过程\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(net, train_iter, loss, trainer)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            train_l = evaluate_loss(net, train_iter, loss)\n",
    "            test_l = evaluate_loss(net, test_iter, loss)\n",
    "            train_loss.append(train_l)\n",
    "            test_loss.append(test_l)\n",
    "            print(f'epoch {epoch+1}, train loss {train_l:.3f}, test loss {test_l:.3f}')\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(range(1, len(train_loss) * 20 + 1, 20), train_loss, label='train')\n",
    "    plt.semilogy(range(1, len(test_loss) * 20 + 1, 20), test_loss, label='test')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print('weight:', net[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用L2正则化\n",
    "train(train_features, test_features,\n",
    "      train_labels, test_labels, penalty_lambda=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题4：使用L2正则化后，模型的参数会发生什么变化？为什么这种变化有助于防止过拟合？\n",
    "\n",
    "提示：\n",
    "- 观察权重的数值分布变化\n",
    "- 分析正则化系数λ的影响\n",
    "- 思考为什么较小的权重有助于泛化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "*Dropout* 是一种在深度学习中广泛使用的正则化技术。\n",
    "与L2正则化通过限制权重大小来防止过拟合不同，\n",
    "Dropout通过在训练过程中随机\"丢弃\"（设置为零）神经元来实现正则化。\n",
    "\n",
    "在训练过程中，对于每个样本，每一层的每个神经元都有概率 $p$ 被暂时从网络中移除。\n",
    "具体来说，如果一个神经元的输出为 $h$，则：\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "\\frac{h}{1-p} & \\text{概率 } 1-p \\text{ (保留)} \\\\\n",
    "0 & \\text{概率 } p \\text{ (丢弃)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "这里的 $\\frac{1}{1-p}$ 是一个缩放因子，用于保持输出的期望值不变。\n",
    "\n",
    "Dropout 可以被理解为在训练时集成了多个不同的子网络。\n",
    "每次前向传播时，由于随机丢弃了一些神经元，\n",
    "实际上是在训练一个新的子网络。\n",
    "这些子网络共享参数，但结构各不相同。\n",
    "在测试时，我们使用完整的网络，但会将所有权重乘以 $(1-p)$ \n",
    "来补偿训练时的缩放。\n",
    "\n",
    "Dropout 的主要优点包括：\n",
    "\n",
    "1. 防止神经元的共适应性（Co-adaptation）\n",
    "   - 因为神经元不能依赖于特定的其他神经元的存在\n",
    "   - 必须学会与随机的神经元子集一起工作\n",
    "\n",
    "2. 提供了一种廉价的模型集成方法\n",
    "   - 每次使用Dropout相当于训练一个新的网络架构\n",
    "   - 最终模型可以看作是多个子网络的集成\n",
    "\n",
    "3. 减少神经元之间的复杂共适应关系\n",
    "   - 每个神经元必须学会更鲁棒的特征\n",
    "   - 不能过分依赖某些特定的特征组合\n",
    "\n",
    "在实践中，Dropout通常在全连接层中使用，\n",
    "丢弃概率 $p$ 通常设置为0.5，\n",
    "而在卷积层中较少使用或使用较小的丢弃概率（如0.1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    device = X.device\n",
    "    # TODO: 实现dropout层\n",
    "    # 1. 如果dropout=1，返回全0张量\n",
    "    # 2. 如果dropout=0，直接返回输入X\n",
    "    # 3. 否则，生成一个与X形状相同的随机掩码(mask)\n",
    "    #    - 使用torch.rand生成随机数，并与dropout比较创建二元掩码\n",
    "    #    - 将X与掩码相乘，并除以(1-dropout)进行缩放\n",
    "    # 注意：过程中device参数需要与X的设备相同\n",
    "    # 在本情况中，所有元素都被丢弃\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(X, device=device)\n",
    "    # 在本情况中，所有元素都被保留\n",
    "    if dropout == 0:\n",
    "        # TODO: 实现dropout=0的情况\n",
    "    # TODO: 实现dropout=其他值的情况\n",
    "    mask = \n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过下面几个例子来**测试`dropout_layer`函数**。\n",
    "我们将输入`X`通过dropout操作，dropout概率分别为0、0.5和1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
    "print(X)\n",
    "print(dropout_layer(X, 0.))\n",
    "print(dropout_layer(X, 0.5))\n",
    "print(dropout_layer(X, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型\n",
    "\n",
    "我们可以将dropout应用于每个隐藏层的输出（在激活函数之后），并且dropout只在训练期间有效。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(RegularizedNetwork, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = True\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.training:\n",
    "            x = dropout_layer(x, self.dropout_rate)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment_regularization():\n",
    "    \"\"\"比较不同正则化方法的效果\"\"\"\n",
    "    \n",
    "    # 实验配置\n",
    "    configs = {\n",
    "        'No Regularization': {'dropout': 0.0},\n",
    "        'Dropout': {'dropout': 0.3},\n",
    "    }\n",
    "    \n",
    "    results = {name: {'train_acc': [], 'test_acc': []} for name in configs.keys()}\n",
    "    n_epochs = 15\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\nTraining with {name}\")\n",
    "        model = RegularizedNetwork(dropout_rate=config['dropout']).to(device)\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(),lr=0.5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader,test_loader = get_data(batch_size=256)\n",
    "        for epoch in range(n_epochs):\n",
    "            # 训练阶段\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                train_total += target.size(0)\n",
    "            \n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            \n",
    "            # 测试阶段\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                    test_total += target.size(0)\n",
    "            \n",
    "            test_acc = 100. * test_correct / test_total\n",
    "            \n",
    "            # 记录结果\n",
    "            results[name]['train_acc'].append(train_acc)\n",
    "            results[name]['test_acc'].append(test_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 训练准确率\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name in results:\n",
    "        plt.plot(results[name]['train_acc'], label=name)\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # 测试准确率\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name in results:\n",
    "        plt.plot(results[name]['test_acc'], label=name)\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 运行实验\n",
    "experiment_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch简洁实现\n",
    "\n",
    "dropout可以作为`nn.Module`类的一个模块来使用，\n",
    "\n",
    "```python\n",
    "nn.Dropout(dropout_rate)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(RegularizedNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate>0 else None\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "experiment_regularization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考题5：Dropout为什么能够起到正则化的作用？训练时和测试时的差异处理有什么意义？\n",
    "\n",
    "提示：\n",
    "- 分析Dropout的集成学习观点\n",
    "- 思考为什么要进行比例缩放\n",
    "- 考虑Dropout对特征依赖的影响"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
